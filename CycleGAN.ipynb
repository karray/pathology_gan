{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloned from https://github.com/eriklindernoren/PyTorch-GAN\n",
    "\n",
    "import os\n",
    "# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID' \n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "import torchvision.models.resnet as resnet\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.utils as vutils\n",
    "import torch.autograd as autograd\n",
    "# from torch.nn.utils import weight_norm\n",
    "\n",
    "from cyclegan.model import Discriminator, Generator\n",
    "from cyclegan.model import weights_init\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import wandb\n",
    "\n",
    "# import torch.backends.cudnn as cudnn\n",
    "# cudnn.benchmark = True\n",
    "\n",
    "from torch.backends import cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "device1 = torch.device(\"cuda:0\")\n",
    "device2 = torch.device(\"cuda:1\")\n",
    "device3 = torch.device(\"cuda:2\")\n",
    "device4 = torch.device(\"cuda:3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEND = 'https://api.telegram.org/bot'+os.environ['TG']+'/'\n",
    "def send(text):\n",
    "    return requests.post(SEND+'sendMessage', json={'chat_id': 80968060, 'text': text}).json()['result']['message_id']\n",
    "\n",
    "def update_msg(text, msg_id):\n",
    "    resp = ''\n",
    "    try:\n",
    "        resp = requests.post(SEND+'editMessageText', json={'chat_id': 80968060, 'text': text, 'message_id': msg_id})\n",
    "    except:\n",
    "        pass\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecayLR:\n",
    "    def __init__(self, epochs, offset, decay_epochs):\n",
    "        epoch_flag = epochs - decay_epochs\n",
    "        assert (epoch_flag > 0), \"Decay must start before the training session ends!\"\n",
    "        self.epochs = epochs\n",
    "        self.offset = offset\n",
    "        self.decay_epochs = decay_epochs\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_epochs) / (\n",
    "                self.epochs - self.decay_epochs)\n",
    "    \n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        assert (max_size > 0), \"Empty buffer or trying to create a black hole. Be careful.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return torch.cat(to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X0_train = np.concatenate([np.load('X0_train_clean_48.npy'), np.load('X0_val_clean_48.npy')])\n",
    "# X1_train = np.concatenate([np.load('X1_train_clean_48.npy'), np.load('X1_val_clean_48.npy')])\n",
    "\n",
    "X0_train = np.load('X0_train_clean_48.npy')\n",
    "X1_train = np.load('X1_train_clean_48.npy')\n",
    "\n",
    "\n",
    "# n = min(X0_train.shape[0], X1_train.shape[0])\n",
    "\n",
    "X0_train = torch.from_numpy((X0_train - .5) / .5)\n",
    "X1_train = torch.from_numpy((X1_train - .5) / .5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader_A = DataLoader(X0_train, batch_size=128, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
    "trainloader_B = DataLoader(X1_train, batch_size=128, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(resnet.ResNet):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sigmoid(self._forward_impl(x))\n",
    "    \n",
    "@torch.no_grad()\n",
    "def eval_G(G, clf, validation_loader, g_device, clf_device):\n",
    "    G.eval()\n",
    "    acc = .0\n",
    "    for i, data in enumerate(validation_loader):\n",
    "        X = data[0].to(g_device)\n",
    "        y = data[1].to(clf_device)\n",
    "        X_g = G(X).to(clf_device)\n",
    "        predicted = torch.round(clf(0.5 * (X_g + 1.0)))\n",
    "        acc+=(predicted == y).sum()/float(predicted.shape[0])     \n",
    "#             acc_g+=(predicted_g == y).sum()/float(predicted_g.shape[0])     \n",
    "    G.train()\n",
    "    return (acc/(i+1)).detach().item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_Clf(model, validation_loader, device):\n",
    "    acc = .0\n",
    "    for i, data in enumerate(validation_loader):\n",
    "        X = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        predicted = torch.round(model(0.5 * (X + 1.0)))\n",
    "        acc+=(predicted == y).sum()/float(predicted.shape[0])       \n",
    "    return (acc/(i+1)).detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0_test = (np.load('X0_val_clean_48.npy') - .5) / .5\n",
    "y0_test = np.load('y0_val_clean_48.npy')\n",
    "\n",
    "X1_test = (np.load('X1_val_clean_48.npy') - .5) / .5\n",
    "y1_test = np.load('y1_val_clean_48.npy')\n",
    "\n",
    "X0_test = torch.from_numpy(X0_test)\n",
    "y0_test = torch.from_numpy(y0_test)\n",
    "X1_test = torch.from_numpy(X1_test)\n",
    "y1_test = torch.from_numpy(y1_test)\n",
    "\n",
    "\n",
    "testloader0 = DataLoader(TensorDataset(X0_test, y0_test), batch_size=128, shuffle=True, num_workers=1, pin_memory=True)\n",
    "testloader1 = DataLoader(TensorDataset(X1_test, y1_test), batch_size=128, shuffle=True, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc for A: 0.8578712940216064\n",
      "Acc for B: 0.88215172290802\n"
     ]
    }
   ],
   "source": [
    "ClfA = ResNet(resnet.BasicBlock, [2, 2, 2, 2], num_classes=1)\n",
    "ClfB = ResNet(resnet.BasicBlock, [2, 2, 2, 2], num_classes=1)\n",
    "ClfA.load_state_dict(torch.load('results/clf_resnet18_48/best_model.pth'))\n",
    "ClfB.load_state_dict(torch.load('results/clf_resnet18_48/best_model.pth'))\n",
    "ClfA = ClfA.to(device3)\n",
    "ClfB = ClfB.to(device4)\n",
    "ClfA.eval()\n",
    "ClfB.eval()\n",
    "\n",
    "print('Acc for A:', eval_Clf(ClfA, testloader0, device3))\n",
    "print('Acc for B:', eval_Clf(ClfB, testloader1, device4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG_B2A = Generator().to(device1)\n",
    "netD_A = Discriminator().to(device1)\n",
    "netG_A2B = Generator().to(device2)\n",
    "netD_B = Discriminator().to(device2)\n",
    "\n",
    "netG_A2B.apply(weights_init)\n",
    "netG_B2A.apply(weights_init)\n",
    "netD_A.apply(weights_init)\n",
    "netD_B.apply(weights_init)\n",
    "\n",
    "cycle_loss1 = torch.nn.L1Loss().to(device1)\n",
    "cycle_loss2 = torch.nn.L1Loss().to(device2)\n",
    "identity_loss1 = torch.nn.L1Loss().to(device1)\n",
    "identity_loss2 = torch.nn.L1Loss().to(device2)\n",
    "adversarial_loss1 = torch.nn.MSELoss().to(device1)\n",
    "adversarial_loss2 = torch.nn.MSELoss().to(device2)\n",
    "\n",
    "# adversarial_loss1 = torch.nn.BCEWithLogitsLoss().to(device1)\n",
    "# adversarial_loss2 = torch.nn.BCEWithLogitsLoss().to(device2)\n",
    "\n",
    "def r1(output, imgs, gamma=0.00001):\n",
    "    grad_real = torch.autograd.grad(outputs=output, inputs=imgs, create_graph=True)[0]\n",
    "    grad_penalty_real = (grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2).mean()\n",
    "    return (gamma) * grad_penalty_real\n",
    "\n",
    "def gradient_penalty(D, real_data, generated_data, device):\n",
    "    batch_size = real_data.shape[0]\n",
    "\n",
    "    # Calculate interpolation\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1)\n",
    "    alpha = alpha.expand_as(real_data).to(device)\n",
    "    interpolated = alpha * real_data + (1 - alpha) * generated_data\n",
    "\n",
    "    # Calculate probability of interpolated examples\n",
    "    dis_interpolated = D(interpolated)\n",
    "    grad_outputs = torch.ones(dis_interpolated.shape).to(device)\n",
    "\n",
    "    # Calculate gradients of probabilities with respect to examples\n",
    "    gradients = autograd.grad(outputs=dis_interpolated, inputs=interpolated,\n",
    "                           grad_outputs=grad_outputs, create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # Gradients have shape (batch_size, num_channels, img_width, img_height),\n",
    "    # so flatten to easily take norm per example in batch\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "#         self.losses['gradient_norm'].append(gradients.norm(2, dim=1).mean().data[0])\n",
    "\n",
    "    # Derivatives of the gradient close to 0 can cause problems because of\n",
    "    # the square root, so manually calculate norm and add epsilon\n",
    "    gradients_norm = ((torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12) - 1) ** 2).mean()\n",
    "    # Return gradient penalty\n",
    "    return 10*gradients_norm\n",
    "\n",
    "# lambda_gp = 10\n",
    "lr = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "# lr = 1e-4\n",
    "# betas = (0, 0.99)\n",
    "# itertools.chain takes a series of iterables and return them as one long iterable.\n",
    "optimizer_G = Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()), lr=lr, betas=betas)\n",
    "optimizer_D_A = Adam(netD_A.parameters(), lr=lr, betas=betas)\n",
    "optimizer_D_B = Adam(netD_B.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "epochs = 200\n",
    "decay_epochs = 100\n",
    "lr_lambda = DecayLR(epochs, 0, decay_epochs).step\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lr_lambda)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=lr_lambda)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=lr_lambda)\n",
    "\n",
    "g_losses = []\n",
    "d_losses_A = []\n",
    "d_losses_B = []\n",
    "acc_a = []\n",
    "acc_b = []\n",
    "\n",
    "identity_losses = []\n",
    "gan_losses = []\n",
    "cycle_losses = []\n",
    "\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marray\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.26 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.13<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">restful-forest-17</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/array/cyclegan_48\" target=\"_blank\">https://wandb.ai/array/cyclegan_48</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/array/cyclegan_48/runs/wlmgp7jt\" target=\"_blank\">https://wandb.ai/array/cyclegan_48/runs/wlmgp7jt</a><br/>\n",
       "                Run data is saved locally in <code>/data/aray/pathology_gan/wandb/run-20210416_162252-wlmgp7jt</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    }
   ],
   "source": [
    "project = 'cyclegan_48'\n",
    "run_name = 'wgan_r1'\n",
    "folder=project+'_'+run_name\n",
    "\n",
    "msg_id = send(folder+': 0')\n",
    "\n",
    "run = wandb.init(project=project)\n",
    "wandb.run.name = run_name\n",
    "wandb.run.save()\n",
    "\n",
    "path = 'results/'+folder\n",
    "path_imgs = path +'/samples'\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "if not os.path.exists(path_imgs):\n",
    "    os.mkdir(path_imgs)\n",
    "if not os.path.exists(path_imgs+'/A'):\n",
    "    os.mkdir(path_imgs+'/A')\n",
    "if not os.path.exists(path_imgs+'/B'):\n",
    "    os.mkdir(path_imgs+'/B')\n",
    "    \n",
    "# print(\n",
    "#     f\" Iter.\\t\"\n",
    "#     f\"LossD A\\t\"\n",
    "#     f\"LossD B\\t\"\n",
    "#     f\" Loss G\\t\"\n",
    "#     f\"Acc B2A\\t\"\n",
    "#     f\"Acc A2B\")\n",
    "\n",
    "    \n",
    "best_acc_A = 0\n",
    "best_acc_B = 0\n",
    "\n",
    "early_stop_cnt = 0\n",
    "\n",
    "iter_A = iter(trainloader_A)\n",
    "iter_B = iter(trainloader_B)\n",
    "\n",
    "total_iter = 100000\n",
    "\n",
    "for i in range(1, total_iter+1):\n",
    "    try:\n",
    "        data_A = next(iter_A)\n",
    "    except:\n",
    "        iter_A = iter(trainloader_A)\n",
    "        data_A = next(iter_A)\n",
    "\n",
    "    try:\n",
    "        data_B = next(iter_B)\n",
    "    except:\n",
    "        iter_B = iter(trainloader_B)\n",
    "        data_B = next(iter_B)\n",
    "\n",
    "    # get batch size data\n",
    "    real_image_A1 = data_A.to(device1)\n",
    "    real_image_B1 = data_B.to(device1)\n",
    "    real_image_A2 = data_A.to(device2)\n",
    "    real_image_B2 = data_B.to(device2)\n",
    "\n",
    "    real_image_A1.requires_grad_()\n",
    "    real_image_B2.requires_grad_()\n",
    "    \n",
    "    batch_size = real_image_A1.size(0)\n",
    "\n",
    "    # real data label is 1, fake data label is 0.\n",
    "    real_label1 = torch.full((batch_size, 1), 1, device=device1, dtype=torch.float32)\n",
    "    fake_label1 = torch.full((batch_size, 1), 0, device=device1, dtype=torch.float32)\n",
    "    real_label2 = torch.full((batch_size, 1), 1, device=device2, dtype=torch.float32)\n",
    "    fake_label2 = torch.full((batch_size, 1), 0, device=device2, dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "\n",
    "    ##############################################\n",
    "    # (1) Update G network: Generators A2B and B2A\n",
    "    ##############################################\n",
    "\n",
    "    # Set G_A and G_B's gradients to zero\n",
    "    optimizer_G.zero_grad()\n",
    "\n",
    "    # Identity loss\n",
    "    # G_B2A(A) should equal A if real A is fed\n",
    "    identity_image_A = netG_B2A(real_image_A1)\n",
    "    loss_identity_A = identity_loss1(identity_image_A, real_image_A1) * 5.0\n",
    "    # G_A2B(B) should equal B if real B is fed\n",
    "    identity_image_B = netG_A2B(real_image_B2)\n",
    "    loss_identity_B = identity_loss2(identity_image_B, real_image_B2) * 5.0\n",
    "\n",
    "    # GAN loss D_A(G_A(A))\n",
    "    fake_image_A = netG_B2A(real_image_B1)\n",
    "    fake_output_A = netD_A(fake_image_A)\n",
    "#     loss_GAN_B2A = adversarial_loss1(fake_output_A, real_label1)\n",
    "    # WGAN\n",
    "    loss_GAN_B2A = -fake_output_A.mean()\n",
    "\n",
    "    # GAN loss D_B(G_B(B))\n",
    "    fake_image_B = netG_A2B(real_image_A2)\n",
    "    fake_output_B = netD_B(fake_image_B)\n",
    "#     loss_GAN_A2B = adversarial_loss2(fake_output_B, real_label2)\n",
    "#     WGAN\n",
    "    loss_GAN_A2B = -fake_output_B.mean()\n",
    "\n",
    "    # Cycle loss\n",
    "    recovered_image_A = netG_B2A(fake_image_B.to(device1))\n",
    "    loss_cycle_ABA = cycle_loss1(recovered_image_A, real_image_A1) * 10.0\n",
    "\n",
    "    recovered_image_B = netG_A2B(fake_image_A.to(device2))\n",
    "    loss_cycle_BAB = cycle_loss2(recovered_image_B, real_image_B2) * 10.0\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    # Combined loss and calculate gradients\n",
    "    errG = loss_identity_A.cpu() + loss_identity_B.cpu() + loss_GAN_A2B.cpu() + loss_GAN_B2A.cpu() + loss_cycle_ABA.cpu() + loss_cycle_BAB.cpu()\n",
    "\n",
    "    g_losses.append(errG.item())\n",
    "    # Calculate gradients for G_A and G_B\n",
    "    errG.backward()\n",
    "    # Update G_A and G_B's weights\n",
    "    optimizer_G.step()\n",
    "\n",
    "    ##############################################\n",
    "    # (2) Update D network: Discriminator A\n",
    "    ##############################################\n",
    "\n",
    "    # Set D_A gradients to zero\n",
    "    optimizer_D_A.zero_grad()\n",
    "\n",
    "    # Real A image loss\n",
    "    real_output_A = netD_A(real_image_A1)\n",
    "#     errD_real_A = adversarial_loss1(real_output_A, real_label1) + r1(real_output_A.sum(), real_image_A1)\n",
    "    # WGAN\n",
    "    errD_real_A = real_output_A.mean() + r1(real_output_A.sum(), real_image_A1)\n",
    "\n",
    "    # Fake A image loss\n",
    "    fake_image_A = fake_A_buffer.push_and_pop(fake_image_A)\n",
    "    fake_output_A = netD_A(fake_image_A.detach().to(device1))\n",
    "#     errD_fake_A = adversarial_loss1(fake_output_A, fake_label1)\n",
    "    # WGAN\n",
    "    errD_fake_A = fake_output_A.mean()\n",
    "\n",
    "    # Combined loss and calculate gradients\n",
    "#     errD_A = (errD_real_A + errD_fake_A) / 2\n",
    "    # WGAN\n",
    "    gp_A = gradient_penalty(netD_A, real_image_A1, netG_B2A(real_image_B1), device1)\n",
    "    errD_A = (errD_fake_A - errD_real_A + gp_A)\n",
    "\n",
    "    d_losses_A.append(errD_A.item())\n",
    "\n",
    "    # Calculate gradients for D_A\n",
    "    errD_A.backward()\n",
    "    # Update D_A weights\n",
    "    optimizer_D_A.step()\n",
    "\n",
    "    ##############################################\n",
    "    # (3) Update D network: Discriminator B\n",
    "    ##############################################\n",
    "\n",
    "    # Set D_B gradients to zero\n",
    "    optimizer_D_B.zero_grad()\n",
    "\n",
    "    # Real B image loss\n",
    "    real_output_B = netD_B(real_image_B2)\n",
    "#     errD_real_B = adversarial_loss2(real_output_B, real_label2) + r1(real_output_B.sum(), real_image_B2)\n",
    "    # WGAN\n",
    "    errD_real_B = real_output_B.mean() + r1(real_output_B.sum(), real_image_B2)\n",
    "\n",
    "    # Fake B image loss\n",
    "    fake_image_B = fake_B_buffer.push_and_pop(fake_image_B)\n",
    "    fake_output_B = netD_B(fake_image_B.detach().to(device2))\n",
    "#     errD_fake_B = adversarial_loss2(fake_output_B, fake_label2)\n",
    "    # WGAN\n",
    "    errD_fake_B = fake_output_B.mean()\n",
    "\n",
    "    # Combined loss and calculate gradients\n",
    "#     errD_B = (errD_real_B + errD_fake_B) / 2\n",
    "    # WGAN\n",
    "    gp_B = gradient_penalty(netD_B, real_image_B2, netG_A2B(real_image_A2), device2)\n",
    "    errD_B = (errD_fake_B - errD_real_B + gp_B)\n",
    "\n",
    "    d_losses_B.append(errD_B.item())\n",
    "\n",
    "    # Calculate gradients for D_B\n",
    "    errD_B.backward()\n",
    "    # Update D_B weights\n",
    "    optimizer_D_B.step()\n",
    "        \n",
    "#     torch.cuda.synchronize()\n",
    "    if i%100 == 0:\n",
    "        update_msg(folder+': '+str(i/total_iter), msg_id)\n",
    "        accA = eval_G(netG_B2A, ClfA, testloader1, device1, device3)\n",
    "        accB = eval_G(netG_A2B, ClfB, testloader0, device2, device4)\n",
    "        \n",
    "        acc_a.append(accA)\n",
    "        acc_b.append(accB)\n",
    "\n",
    "#         print(\n",
    "#             f\"{i:06d}\\t\"\n",
    "#             f\"{d_losses_A[-1]:2.4f}\\t\"\n",
    "#             f\"{d_losses_B[-1]:2.4f}\\t\"\n",
    "#             f\"{g_losses[-1]:2.4f}\\t\"\n",
    "#             f\"{accA:2.4f}\\t\"\n",
    "#             f\"{accB:2.4f}\")\n",
    "        \n",
    "        wandb.log(\n",
    "            {\n",
    "                \"d_losses_A\": d_losses_A[-1],\n",
    "                \"d_losses_B\": d_losses_B[-1],\n",
    "                \"g_losses\": g_losses[-1],\n",
    "                \"accA\": accA,\n",
    "                \"accB\": accB,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "        if accA > best_acc_A:\n",
    "            best_acc_A = accA\n",
    "            # save last check pointing\n",
    "            torch.save(netG_B2A.state_dict(), path+\"/netG_B2A.pth\")\n",
    "            torch.save(netD_A.state_dict(), path+\"/netD_A.pth\")\n",
    "            wandb.run.summary[\"best_acc_A\"] = accA\n",
    "        if accB > best_acc_B:\n",
    "            best_acc_B = accB\n",
    "            # save last check pointing\n",
    "            torch.save(netG_A2B.state_dict(), path+\"/netG_A2B.pth\")\n",
    "            torch.save(netD_B.state_dict(), path+\"/netD_B.pth\")\n",
    "            wandb.run.summary[\"best_acc_B\"] = accB\n",
    "    \n",
    "#     if i%1000 == 0:\n",
    "#         fake_image_A = 0.5 * (netG_B2A(real_image_B1).data[:16] + 1.0)\n",
    "#         fake_image_B = 0.5 * (netG_A2B(real_image_A2).data[:16] + 1.0)\n",
    "\n",
    "#         vutils.save_image(fake_image_A.detach(),\n",
    "#                         path_imgs+f\"/A/{i:06d}_fake.png\",\n",
    "#                         normalize=True)\n",
    "#         vutils.save_image(fake_image_B.detach(),\n",
    "#                         path_imgs+f\"/B/{i:06d}_fake.png\",\n",
    "#                         normalize=True)\n",
    "\n",
    "    if i%1000 == 0:\n",
    "        # Update learning rates\n",
    "        lr_scheduler_G.step()\n",
    "        lr_scheduler_D_A.step()\n",
    "        lr_scheduler_D_B.step()\n",
    "\n",
    "send(folder+' done')\n",
    "\n",
    "wandb.save(path+\"/netG_B2A.pth\")\n",
    "wandb.save(path+\"/netD_A.pth\")\n",
    "wandb.save(path+\"/netG_A2B.pth\")\n",
    "wandb.save(path+\"/netD_B.pth\")\n",
    "    \n",
    "torch.save(netG_A2B.state_dict(), path+\"/netG_A2B_last.pth\")\n",
    "torch.save(netG_B2A.state_dict(), path+\"/netG_B2A_last.pth\")\n",
    "torch.save(netD_A.state_dict(), path+\"/netD_A_last.pth\")\n",
    "torch.save(netD_B.state_dict(), path+\"/netD_B_last.pth\")\n",
    "\n",
    "np.save(path+'/d_losses_A.npy', d_losses_A)\n",
    "np.save(path+'/d_losses_B.npy', d_losses_B)\n",
    "np.save(path+'/g_losses.npy', g_losses)\n",
    "np.save(path+'/acc_a.npy', acc_a)\n",
    "np.save(path+'/acc_b.npy', acc_b)\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "from cyclegan.model import Generator as CycleGANGenerator\n",
    "from cyclegan.model import Discriminator as CycleGANDiscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ReflectionPad2d-1           [128, 3, 54, 54]               0\n",
      "            Conv2d-2          [128, 64, 48, 48]           9,472\n",
      "    InstanceNorm2d-3          [128, 64, 48, 48]               0\n",
      "              ReLU-4          [128, 64, 48, 48]               0\n",
      "            Conv2d-5         [128, 128, 24, 24]          73,856\n",
      "    InstanceNorm2d-6         [128, 128, 24, 24]               0\n",
      "              ReLU-7         [128, 128, 24, 24]               0\n",
      "            Conv2d-8         [128, 256, 12, 12]         295,168\n",
      "    InstanceNorm2d-9         [128, 256, 12, 12]               0\n",
      "             ReLU-10         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-11         [128, 256, 14, 14]               0\n",
      "           Conv2d-12         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-13         [128, 256, 12, 12]               0\n",
      "             ReLU-14         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-15         [128, 256, 14, 14]               0\n",
      "           Conv2d-16         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-17         [128, 256, 12, 12]               0\n",
      "    ResidualBlock-18         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-19         [128, 256, 14, 14]               0\n",
      "           Conv2d-20         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-21         [128, 256, 12, 12]               0\n",
      "             ReLU-22         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-23         [128, 256, 14, 14]               0\n",
      "           Conv2d-24         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-25         [128, 256, 12, 12]               0\n",
      "    ResidualBlock-26         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-27         [128, 256, 14, 14]               0\n",
      "           Conv2d-28         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-29         [128, 256, 12, 12]               0\n",
      "             ReLU-30         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-31         [128, 256, 14, 14]               0\n",
      "           Conv2d-32         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-33         [128, 256, 12, 12]               0\n",
      "    ResidualBlock-34         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-35         [128, 256, 14, 14]               0\n",
      "           Conv2d-36         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-37         [128, 256, 12, 12]               0\n",
      "             ReLU-38         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-39         [128, 256, 14, 14]               0\n",
      "           Conv2d-40         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-41         [128, 256, 12, 12]               0\n",
      "    ResidualBlock-42         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-43         [128, 256, 14, 14]               0\n",
      "           Conv2d-44         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-45         [128, 256, 12, 12]               0\n",
      "             ReLU-46         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-47         [128, 256, 14, 14]               0\n",
      "           Conv2d-48         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-49         [128, 256, 12, 12]               0\n",
      "    ResidualBlock-50         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-51         [128, 256, 14, 14]               0\n",
      "           Conv2d-52         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-53         [128, 256, 12, 12]               0\n",
      "             ReLU-54         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-55         [128, 256, 14, 14]               0\n",
      "           Conv2d-56         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-57         [128, 256, 12, 12]               0\n",
      "    ResidualBlock-58         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-59         [128, 256, 14, 14]               0\n",
      "           Conv2d-60         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-61         [128, 256, 12, 12]               0\n",
      "             ReLU-62         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-63         [128, 256, 14, 14]               0\n",
      "           Conv2d-64         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-65         [128, 256, 12, 12]               0\n",
      "    ResidualBlock-66         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-67         [128, 256, 14, 14]               0\n",
      "           Conv2d-68         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-69         [128, 256, 12, 12]               0\n",
      "             ReLU-70         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-71         [128, 256, 14, 14]               0\n",
      "           Conv2d-72         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-73         [128, 256, 12, 12]               0\n",
      "    ResidualBlock-74         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-75         [128, 256, 14, 14]               0\n",
      "           Conv2d-76         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-77         [128, 256, 12, 12]               0\n",
      "             ReLU-78         [128, 256, 12, 12]               0\n",
      "  ReflectionPad2d-79         [128, 256, 14, 14]               0\n",
      "           Conv2d-80         [128, 256, 12, 12]         590,080\n",
      "   InstanceNorm2d-81         [128, 256, 12, 12]               0\n",
      "    ResidualBlock-82         [128, 256, 12, 12]               0\n",
      "         Upsample-83         [128, 256, 24, 24]               0\n",
      "           Conv2d-84         [128, 128, 24, 24]         295,040\n",
      "   InstanceNorm2d-85         [128, 128, 24, 24]               0\n",
      "             ReLU-86         [128, 128, 24, 24]               0\n",
      "         Upsample-87         [128, 128, 48, 48]               0\n",
      "           Conv2d-88          [128, 64, 48, 48]          73,792\n",
      "   InstanceNorm2d-89          [128, 64, 48, 48]               0\n",
      "             ReLU-90          [128, 64, 48, 48]               0\n",
      "  ReflectionPad2d-91          [128, 64, 54, 54]               0\n",
      "           Conv2d-92           [128, 3, 48, 48]           9,411\n",
      "             Tanh-93           [128, 3, 48, 48]               0\n",
      "================================================================\n",
      "Total params: 11,378,179\n",
      "Trainable params: 11,378,179\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.38\n",
      "Forward/backward pass size (MB): 4866.29\n",
      "Params size (MB): 43.40\n",
      "Estimated Total Size (MB): 4913.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(CycleGANGenerator().cuda(), (3, 48, 48), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [128, 64, 24, 24]           3,136\n",
      "         LeakyReLU-2          [128, 64, 24, 24]               0\n",
      "            Conv2d-3         [128, 128, 12, 12]         131,200\n",
      "    InstanceNorm2d-4         [128, 128, 12, 12]               0\n",
      "         LeakyReLU-5         [128, 128, 12, 12]               0\n",
      "            Conv2d-6           [128, 256, 6, 6]         524,544\n",
      "    InstanceNorm2d-7           [128, 256, 6, 6]               0\n",
      "         LeakyReLU-8           [128, 256, 6, 6]               0\n",
      "            Conv2d-9           [128, 512, 5, 5]       2,097,664\n",
      "   InstanceNorm2d-10           [128, 512, 5, 5]               0\n",
      "        LeakyReLU-11           [128, 512, 5, 5]               0\n",
      "           Conv2d-12             [128, 1, 4, 4]           8,193\n",
      "================================================================\n",
      "Total params: 2,764,737\n",
      "Trainable params: 2,764,737\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.38\n",
      "Forward/backward pass size (MB): 190.52\n",
      "Params size (MB): 10.55\n",
      "Estimated Total Size (MB): 204.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(CycleGANDiscriminator().cuda(), (3, 48, 48), batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
