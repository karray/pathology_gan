{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID' \n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3,4,5,6'\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import torchvision.models.resnet as resnet\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.utils as vutils\n",
    "import torch.autograd as autograd\n",
    "# from torch.nn.utils import weight_norm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import torch.backends.cudnn as cudnn\n",
    "# cudnn.benchmark = True\n",
    "\n",
    "from torch.backends import cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "device1 = torch.device(\"cuda:0\")\n",
    "device2 = torch.device(\"cuda:1\")\n",
    "device3 = torch.device(\"cuda:2\")\n",
    "device4 = torch.device(\"cuda:3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        x = F.avg_pool2d(x, x.size()[2:])\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Initial convolution block\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(3, 64, 7),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Downsampling\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Residual blocks\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "\n",
    "            # Upsampling\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "#             nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "#             nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Output layer\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, 3, 7),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.res = nn.Sequential(nn.ReflectionPad2d(1),\n",
    "                                 nn.Conv2d(in_channels, in_channels, 3),\n",
    "                                 nn.InstanceNorm2d(in_channels),\n",
    "                                 nn.ReLU(inplace=True),\n",
    "                                 nn.ReflectionPad2d(1),\n",
    "                                 nn.Conv2d(in_channels, in_channels, 3),\n",
    "                                 nn.InstanceNorm2d(in_channels)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.res(x)\n",
    "    \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "        \n",
    "class DecayLR:\n",
    "    def __init__(self, epochs, offset, decay_epochs):\n",
    "        epoch_flag = epochs - decay_epochs\n",
    "        assert (epoch_flag > 0), \"Decay must start before the training session ends!\"\n",
    "        self.epochs = epochs\n",
    "        self.offset = offset\n",
    "        self.decay_epochs = decay_epochs\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_epochs) / (\n",
    "                self.epochs - self.decay_epochs)\n",
    "    \n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        assert (max_size > 0), \"Empty buffer or trying to create a black hole. Be careful.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return torch.cat(to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(size):\n",
    "#     hfs = 48 # half full size 96/2=48\n",
    "#     hs = size//2\n",
    "#     start = hfs-hs\n",
    "#     end = hfs+hs\n",
    "    \n",
    "#     X = (np.load('X_clean.npy')[:, start:end, start:end] - 127.5) / 127.5\n",
    "#     X = np.moveaxis(X, -1, 1).astype(np.float32)\n",
    "#     y = np.load('Y_clean.npy').reshape(-1,1).astype(np.float32)\n",
    "#     wsi = np.load('WSI_clean.npy')\n",
    "    \n",
    "# #     X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "# #     X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "    \n",
    "# #     X_0 = torch.from_numpy(X[WSI==0])\n",
    "# #     X_1 = torch.from_numpy(X[WSI==1])\n",
    "# #     Y_0 = torch.from_numpy(Y[WSI==0])\n",
    "# #     Y_1 = torch.from_numpy(Y[WSI==1])\n",
    "# #     return X_0, Y_0, X_1, Y_1\n",
    "\n",
    "#     return X, y, wsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y, wsi = load_data(48)\n",
    "\n",
    "# X_test = torch.from_numpy(X)\n",
    "# y_test = torch.from_numpy(y)\n",
    "\n",
    "# idx0 = wsi==0\n",
    "# idx1 = wsi==1\n",
    "\n",
    "# _, X0_test, _, y0_test = train_test_split(X_test[idx0], y_test[idx0], test_size=0.2, random_state=0)\n",
    "# _, X1_test, _, y1_test = train_test_split(X_test[idx1], y_test[idx1], test_size=0.2, random_state=0)\n",
    "\n",
    "# # TODO: RANDOM SAMPLE\n",
    "\n",
    "# X0 = X[idx0]\n",
    "# X1 = X[idx1]\n",
    "# y0 = y[idx0]\n",
    "# y1 = y[idx1]\n",
    "\n",
    "# n = min(X0.shape[0], X1.shape[0])\n",
    "\n",
    "\n",
    "# X0 = torch.from_numpy(X0[:n])\n",
    "# X1 = torch.from_numpy(X1[:n])\n",
    "\n",
    "# trainloader = DataLoader(TensorDataset(X0, X1), batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X0_train = np.concatenate([np.load('X0_train_clean_48.npy'), np.load('X0_val_clean_48.npy')])\n",
    "# X1_train = np.concatenate([np.load('X1_train_clean_48.npy'), np.load('X1_val_clean_48.npy')])\n",
    "\n",
    "X0_train = np.load('X0_train_clean_48.npy')\n",
    "X1_train = np.load('X1_train_clean_48.npy')\n",
    "\n",
    "\n",
    "# n = min(X0_train.shape[0], X1_train.shape[0])\n",
    "\n",
    "X0_train = torch.from_numpy((X0_train - .5) / .5)\n",
    "X1_train = torch.from_numpy((X1_train - .5) / .5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader_A = DataLoader(X0_train, batch_size=128, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
    "trainloader_B = DataLoader(X1_train, batch_size=128, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(resnet.ResNet):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sigmoid(self._forward_impl(x))\n",
    "    \n",
    "@torch.no_grad()\n",
    "def eval_G(G, clf, validation_loader, g_device, clf_device):\n",
    "    G.eval()\n",
    "    acc = .0\n",
    "    for i, data in enumerate(validation_loader):\n",
    "        X = data[0].to(g_device)\n",
    "        y = data[1].to(clf_device)\n",
    "        X_g = G(X).to(clf_device)\n",
    "        predicted = torch.round(clf(0.5 * (X_g + 1.0)))\n",
    "        acc+=(predicted == y).sum()/float(predicted.shape[0])     \n",
    "#             acc_g+=(predicted_g == y).sum()/float(predicted_g.shape[0])     \n",
    "    G.train()\n",
    "    return (acc/(i+1)).detach().item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_Clf(model, validation_loader, device):\n",
    "    acc = .0\n",
    "    for i, data in enumerate(validation_loader):\n",
    "        X = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        predicted = torch.round(model(0.5 * (X + 1.0)))\n",
    "        acc+=(predicted == y).sum()/float(predicted.shape[0])       \n",
    "    return (acc/(i+1)).detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0_test = (np.load('X0_val_clean_48.npy') - .5) / .5\n",
    "y0_test = np.load('y0_val_clean_48.npy')\n",
    "\n",
    "X1_test = (np.load('X1_val_clean_48.npy') - .5) / .5\n",
    "y1_test = np.load('y1_val_clean_48.npy')\n",
    "\n",
    "X0_test = torch.from_numpy(X0_test)\n",
    "y0_test = torch.from_numpy(y0_test)\n",
    "X1_test = torch.from_numpy(X1_test)\n",
    "y1_test = torch.from_numpy(y1_test)\n",
    "\n",
    "\n",
    "testloader0 = DataLoader(TensorDataset(X0_test, y0_test), batch_size=128, shuffle=True, num_workers=1, pin_memory=True)\n",
    "testloader1 = DataLoader(TensorDataset(X1_test, y1_test), batch_size=128, shuffle=True, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc for A: 0.857907772064209\n",
      "Acc for B: 0.88215172290802\n"
     ]
    }
   ],
   "source": [
    "ClfA = ResNet(resnet.BasicBlock, [2, 2, 2, 2], num_classes=1)\n",
    "ClfB = ResNet(resnet.BasicBlock, [2, 2, 2, 2], num_classes=1)\n",
    "ClfA.load_state_dict(torch.load('results/clf_resnet18_48/best_model.pth'))\n",
    "ClfB.load_state_dict(torch.load('results/clf_resnet18_48/best_model.pth'))\n",
    "ClfA = ClfA.to(device3)\n",
    "ClfB = ClfB.to(device4)\n",
    "ClfA.eval()\n",
    "ClfB.eval()\n",
    "\n",
    "print('Acc for A:', eval_Clf(ClfA, testloader0, device3))\n",
    "print('Acc for B:', eval_Clf(ClfB, testloader1, device4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG_B2A = Generator().to(device1)\n",
    "netD_A = Discriminator().to(device1)\n",
    "netG_A2B = Generator().to(device2)\n",
    "netD_B = Discriminator().to(device2)\n",
    "\n",
    "netG_A2B.apply(weights_init)\n",
    "netG_B2A.apply(weights_init)\n",
    "netD_A.apply(weights_init)\n",
    "netD_B.apply(weights_init)\n",
    "\n",
    "cycle_loss1 = torch.nn.L1Loss().to(device1)\n",
    "cycle_loss2 = torch.nn.L1Loss().to(device2)\n",
    "identity_loss1 = torch.nn.L1Loss().to(device1)\n",
    "identity_loss2 = torch.nn.L1Loss().to(device2)\n",
    "adversarial_loss1 = torch.nn.MSELoss().to(device1)\n",
    "adversarial_loss2 = torch.nn.MSELoss().to(device2)\n",
    "\n",
    "def gradient_penalty(D, real_data, generated_data, device):\n",
    "    batch_size = real_data.shape[0]\n",
    "\n",
    "    # Calculate interpolation\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1)\n",
    "    alpha = alpha.expand_as(real_data).to(device)\n",
    "    interpolated = alpha * real_data + (1 - alpha) * generated_data\n",
    "\n",
    "    # Calculate probability of interpolated examples\n",
    "    dis_interpolated = D(interpolated)\n",
    "    grad_outputs = torch.ones(dis_interpolated.shape).to(device)\n",
    "\n",
    "    # Calculate gradients of probabilities with respect to examples\n",
    "    gradients = autograd.grad(outputs=dis_interpolated, inputs=interpolated,\n",
    "                           grad_outputs=grad_outputs, create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # Gradients have shape (batch_size, num_channels, img_width, img_height),\n",
    "    # so flatten to easily take norm per example in batch\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "#         self.losses['gradient_norm'].append(gradients.norm(2, dim=1).mean().data[0])\n",
    "\n",
    "    # Derivatives of the gradient close to 0 can cause problems because of\n",
    "    # the square root, so manually calculate norm and add epsilon\n",
    "    gradients_norm = ((torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12) - 1) ** 2).mean()\n",
    "    # Return gradient penalty\n",
    "    return 10*gradients_norm\n",
    "\n",
    "# lambda_gp = 10\n",
    "lr = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "# itertools.chain takes a series of iterables and return them as one long iterable.\n",
    "optimizer_G = Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()), lr=lr, betas=betas)\n",
    "optimizer_D_A = Adam(netD_A.parameters(), lr=lr, betas=betas)\n",
    "optimizer_D_B = Adam(netD_B.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "epochs = 200\n",
    "decay_epochs = 100\n",
    "lr_lambda = DecayLR(epochs, 0, decay_epochs).step\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lr_lambda)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=lr_lambda)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=lr_lambda)\n",
    "\n",
    "g_losses = []\n",
    "d_losses_A = []\n",
    "d_losses_B = []\n",
    "acc_a = []\n",
    "acc_b = []\n",
    "\n",
    "identity_losses = []\n",
    "gan_losses = []\n",
    "cycle_losses = []\n",
    "\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter.\tLossD A\tLossD B\t Loss G\tAcc B2A\tAcc A2B\n",
      "000100\t-5.7026\t2.0082\t10.1286\t0.5369\t0.5125\n",
      "000200\t-3.1013\t-2.3119\t2.5310\t0.4383\t0.5119\n",
      "000300\t-3.0927\t0.3767\t6.8666\t0.4444\t0.5132\n",
      "000400\t-2.0858\t0.9277\t-2.8063\t0.4057\t0.5139\n",
      "000500\t-1.8311\t-3.2720\t-5.1563\t0.4737\t0.5377\n",
      "000600\t-1.1130\t0.2090\t-6.9285\t0.4546\t0.6164\n",
      "000700\t-1.4422\t-2.0538\t-8.5690\t0.6084\t0.5994\n",
      "000800\t-1.3999\t-1.6005\t-7.7282\t0.7228\t0.5878\n",
      "000900\t-0.4474\t-1.4726\t-8.4106\t0.5143\t0.5713\n",
      "001000\t0.0110\t-0.6658\t-12.8075\t0.5129\t0.5943\n",
      "001100\t-0.5831\t-1.5291\t-8.7558\t0.6513\t0.5617\n",
      "001200\t-0.4791\t-0.5447\t-12.8200\t0.4771\t0.5839\n",
      "001300\t-0.2279\t-0.9182\t-11.6370\t0.7073\t0.6353\n",
      "001400\t-0.9444\t-0.4438\t-11.4723\t0.6580\t0.5958\n",
      "001500\t-0.8111\t-0.2126\t-9.7701\t0.6200\t0.6100\n",
      "001600\t-0.4868\t-0.9370\t-9.6087\t0.5988\t0.5029\n",
      "001700\t-0.6085\t-0.5380\t-8.1101\t0.6341\t0.5379\n",
      "001800\t-0.1421\t-0.4311\t-6.6050\t0.7016\t0.5408\n",
      "001900\t-0.1456\t-0.7224\t-8.7820\t0.4777\t0.6790\n",
      "002000\t-0.1409\t-0.7084\t-8.7877\t0.6196\t0.6834\n",
      "002100\t-0.6341\t-0.3486\t-7.4337\t0.7072\t0.6544\n",
      "002200\t-0.8166\t-0.5064\t-6.4958\t0.6875\t0.6822\n",
      "002300\t-0.4253\t-0.9635\t-6.4398\t0.4880\t0.6373\n",
      "002400\t-0.8526\t-0.4865\t-7.6156\t0.5593\t0.6039\n",
      "002500\t-0.7251\t-0.6865\t-5.6031\t0.6953\t0.6620\n",
      "002600\t-0.3870\t-0.4084\t-6.1314\t0.6876\t0.6754\n",
      "002700\t-0.4124\t-0.5402\t-5.8075\t0.7512\t0.6246\n",
      "002800\t-0.5249\t-0.7271\t-5.3677\t0.6936\t0.6975\n",
      "002900\t-0.5702\t-0.5340\t-6.1606\t0.6745\t0.6117\n",
      "003000\t-0.5422\t-0.5409\t-5.6395\t0.6831\t0.6876\n",
      "003100\t-0.7107\t-0.8500\t-6.3762\t0.6656\t0.6674\n",
      "003200\t-0.5015\t-0.5956\t-5.2742\t0.7205\t0.6717\n",
      "003300\t-0.6682\t-0.6141\t-5.4756\t0.4673\t0.6742\n",
      "003400\t-0.4607\t-0.6730\t-5.8294\t0.5810\t0.5150\n",
      "003500\t-0.2125\t-0.6857\t-6.0982\t0.4616\t0.5635\n",
      "003600\t-0.6353\t-0.6113\t-6.6462\t0.6808\t0.7107\n",
      "003700\t-0.4146\t-0.4194\t-6.5670\t0.6446\t0.7017\n",
      "003800\t-0.4534\t-0.8351\t-5.9437\t0.7059\t0.6525\n",
      "003900\t-0.6402\t-0.6749\t-5.3879\t0.7575\t0.5770\n",
      "004000\t-0.6434\t-0.2012\t-6.8954\t0.6682\t0.6093\n",
      "004100\t-0.2820\t-0.5283\t-7.6905\t0.6988\t0.5886\n",
      "004200\t-0.4924\t-0.7071\t-6.9806\t0.6619\t0.6571\n",
      "004300\t-0.5242\t-0.4179\t-6.5032\t0.7935\t0.6838\n",
      "004400\t-0.4709\t-0.6250\t-6.1217\t0.5313\t0.6040\n",
      "004500\t-0.3681\t-0.4851\t-6.7286\t0.7042\t0.6127\n",
      "004600\t-0.5293\t-0.6716\t-7.0786\t0.6661\t0.6245\n",
      "004700\t-0.3372\t-0.4844\t-7.0046\t0.5972\t0.6827\n",
      "004800\t-1.8950\t-0.4640\t-3.7598\t0.6086\t0.6322\n",
      "004900\t-0.3336\t-0.4946\t-8.0588\t0.6349\t0.7072\n",
      "005000\t-0.8399\t-0.4416\t-7.4820\t0.7700\t0.6824\n",
      "005100\t-0.3736\t-0.5023\t-7.2864\t0.7870\t0.6640\n",
      "005200\t-0.3418\t-0.5056\t-7.9800\t0.6131\t0.6371\n",
      "005300\t-0.3541\t-0.3348\t-8.0264\t0.6403\t0.6685\n",
      "005400\t-0.2890\t-0.4800\t-8.2285\t0.7300\t0.7037\n",
      "005500\t-0.4849\t-0.3569\t-7.8802\t0.7855\t0.6946\n",
      "005600\t-0.2807\t-0.5605\t-8.3238\t0.7903\t0.6728\n",
      "005700\t-0.3446\t-0.4022\t-8.0929\t0.4961\t0.6245\n",
      "005800\t-0.7392\t-0.5297\t-7.0609\t0.6406\t0.6367\n",
      "005900\t-0.3890\t-0.5817\t-8.3174\t0.5754\t0.6712\n",
      "006000\t-0.1453\t-0.4151\t-8.2465\t0.5793\t0.5850\n",
      "006100\t-0.3501\t-0.3741\t-8.2746\t0.6245\t0.6456\n",
      "006200\t-0.1381\t-0.4122\t-8.8106\t0.5657\t0.7053\n",
      "006300\t-0.3305\t-0.4901\t-8.5834\t0.7904\t0.6781\n",
      "006400\t-0.2339\t-0.2147\t-7.8031\t0.6745\t0.5940\n",
      "006500\t-0.2638\t-0.1538\t-8.4209\t0.5537\t0.6938\n",
      "006600\t-0.3717\t-0.6209\t-8.7255\t0.6932\t0.7115\n",
      "006700\t-0.5256\t-0.3825\t-8.7860\t0.6797\t0.6912\n",
      "006800\t-0.3623\t-0.4017\t-9.5262\t0.7833\t0.6728\n",
      "006900\t-0.3723\t-0.4881\t-8.6706\t0.4995\t0.6621\n",
      "007000\t-0.2991\t-0.3111\t-9.7592\t0.6672\t0.6441\n",
      "007100\t-0.4547\t-0.4454\t-9.9217\t0.6902\t0.6918\n",
      "007200\t-0.3290\t-0.2678\t-9.7142\t0.7661\t0.6769\n",
      "007300\t-0.2931\t-0.6225\t-9.4007\t0.7597\t0.6801\n",
      "007400\t-0.0561\t-0.1489\t-10.6326\t0.6075\t0.6669\n",
      "007500\t-0.1817\t-0.1991\t-10.4999\t0.7292\t0.6600\n",
      "007600\t-0.2690\t-0.4282\t-9.7628\t0.6389\t0.6807\n",
      "007700\t-0.4349\t-0.3212\t-10.3604\t0.7319\t0.6684\n",
      "007800\t-0.3286\t-0.0328\t-10.2562\t0.6831\t0.7132\n",
      "007900\t-0.2506\t-0.3211\t-10.2391\t0.6356\t0.6697\n",
      "008000\t-0.3046\t-0.3731\t-9.9106\t0.7504\t0.7185\n",
      "008100\t-0.2180\t-0.3566\t-10.1264\t0.6882\t0.6277\n",
      "008200\t-0.4169\t-0.5530\t-10.0196\t0.6571\t0.6331\n",
      "008300\t-0.2772\t-0.5196\t-9.7841\t0.6201\t0.7237\n",
      "008400\t-0.3077\t-0.3518\t-9.8668\t0.5494\t0.6982\n",
      "008500\t-0.4694\t-0.4698\t-9.6003\t0.7303\t0.6825\n",
      "008600\t-0.3594\t-0.4815\t-10.5260\t0.5237\t0.6543\n",
      "008700\t-0.4264\t-0.3737\t-9.6199\t0.6923\t0.6817\n",
      "008800\t-0.3820\t-0.2686\t-10.7093\t0.7603\t0.7110\n",
      "008900\t-0.1069\t-0.2345\t-11.3397\t0.4916\t0.7173\n",
      "009000\t-0.4543\t-0.3599\t-10.4930\t0.6711\t0.6481\n",
      "009100\t-0.2957\t-0.3409\t-11.8469\t0.7382\t0.6795\n",
      "009200\t-0.3468\t-0.4913\t-10.3827\t0.5397\t0.6708\n",
      "009300\t-0.3398\t-0.3055\t-10.1118\t0.7562\t0.7138\n",
      "009400\t-0.4939\t-0.4855\t-10.7240\t0.7482\t0.7057\n",
      "009500\t-0.1123\t-0.2297\t-11.3687\t0.6441\t0.6397\n",
      "009600\t-0.1926\t-0.1616\t-10.1899\t0.8000\t0.6906\n",
      "009700\t-0.2557\t-0.3215\t-11.3457\t0.6966\t0.6908\n",
      "009800\t-0.3642\t0.0363\t-10.6625\t0.6570\t0.6475\n",
      "009900\t-0.1396\t-0.3183\t-11.0028\t0.6678\t0.7142\n",
      "010000\t-0.3384\t-0.1847\t-10.2879\t0.7685\t0.7116\n",
      "010100\t-0.5574\t-0.3898\t-10.5159\t0.6739\t0.6269\n",
      "010200\t-0.2512\t-0.6223\t-10.4854\t0.5931\t0.6595\n",
      "010300\t-0.3024\t-0.2612\t-10.0434\t0.7660\t0.6836\n",
      "010400\t-0.3218\t-0.3323\t-11.3219\t0.6561\t0.6809\n",
      "010500\t-0.4325\t-0.3511\t-10.6257\t0.6536\t0.6499\n",
      "010600\t-0.5395\t-0.4423\t-10.6830\t0.5063\t0.6642\n",
      "010700\t-0.2255\t0.0695\t-11.7158\t0.7037\t0.6888\n",
      "010800\t-0.2429\t-0.3354\t-11.3041\t0.6349\t0.6968\n",
      "010900\t-0.3388\t-0.1881\t-11.7421\t0.7229\t0.6685\n",
      "011000\t-0.4995\t-0.3688\t-12.0554\t0.6894\t0.6769\n",
      "011100\t-0.5555\t-0.6558\t-10.9619\t0.5902\t0.6128\n",
      "011200\t-0.2548\t-0.4572\t-10.9919\t0.6703\t0.6977\n",
      "011300\t-0.1763\t-0.4755\t-11.1551\t0.4527\t0.7093\n",
      "011400\t-0.3632\t-0.5556\t-11.3300\t0.5710\t0.7019\n",
      "011500\t-0.3886\t-0.3904\t-10.7045\t0.7707\t0.6589\n",
      "011600\t-0.1281\t-0.1906\t-12.4679\t0.6552\t0.7164\n",
      "011700\t-0.1383\t-0.0687\t-12.2191\t0.6123\t0.6478\n",
      "011800\t-0.0313\t-0.2644\t-12.4223\t0.5667\t0.7173\n",
      "011900\t-0.1034\t-0.4737\t-11.5498\t0.7585\t0.5989\n",
      "012000\t-0.1977\t-0.3525\t-12.1892\t0.5416\t0.6399\n",
      "012100\t-0.1697\t-0.3940\t-12.0663\t0.6179\t0.6870\n",
      "012200\t-0.3115\t-0.3317\t-11.4190\t0.7749\t0.6630\n",
      "012300\t-0.1353\t-0.3973\t-11.8521\t0.7022\t0.6693\n",
      "012400\t-0.2975\t-0.3289\t-12.3234\t0.7541\t0.6915\n",
      "012500\t-0.3564\t-0.3982\t-11.6925\t0.6619\t0.7184\n",
      "012600\t-0.3477\t-0.4345\t-11.9385\t0.7709\t0.7023\n",
      "012700\t-0.3779\t-0.4329\t-11.6778\t0.7655\t0.6554\n",
      "012800\t-0.3769\t-0.3555\t-12.1261\t0.7241\t0.6649\n",
      "012900\t-0.2277\t-0.2657\t-12.4864\t0.7597\t0.6568\n",
      "013000\t-0.1048\t-0.2615\t-12.3505\t0.7693\t0.7105\n",
      "013100\t-0.3435\t-0.3620\t-11.7619\t0.7314\t0.6967\n",
      "013200\t-0.4968\t-0.2364\t-12.9544\t0.7557\t0.7233\n",
      "013300\t-0.3305\t-0.4013\t-12.5410\t0.7000\t0.6095\n",
      "013400\t-0.3193\t-0.6758\t-13.5579\t0.7464\t0.7034\n",
      "013500\t-0.0769\t-0.4995\t-12.4429\t0.6494\t0.7230\n",
      "013600\t-0.1791\t-0.3042\t-12.7195\t0.6412\t0.6114\n",
      "013700\t-0.2006\t-0.2490\t-13.8623\t0.6781\t0.6877\n",
      "013800\t-0.2802\t-0.3175\t-12.2540\t0.6595\t0.6628\n",
      "013900\t-0.3410\t-0.3335\t-12.9887\t0.7083\t0.6927\n",
      "014000\t-0.1040\t-0.4160\t-12.7564\t0.5472\t0.7271\n",
      "014100\t-0.2219\t-0.2827\t-12.8082\t0.5931\t0.6999\n",
      "014200\t-0.3214\t-0.2799\t-12.8278\t0.7745\t0.6728\n",
      "014300\t-0.2527\t-0.2535\t-12.7547\t0.6613\t0.6147\n",
      "014400\t-0.1673\t-0.3239\t-13.0654\t0.7439\t0.6964\n",
      "014500\t-0.2932\t-0.3379\t-12.6802\t0.7149\t0.7354\n",
      "014600\t-0.3115\t-0.1901\t-13.3582\t0.6667\t0.6836\n",
      "014700\t-0.2926\t-0.3378\t-14.0934\t0.7376\t0.7151\n",
      "014800\t-0.2380\t-0.3423\t-13.1959\t0.7025\t0.6257\n",
      "014900\t-0.2277\t0.0515\t-13.6440\t0.6966\t0.6765\n",
      "015000\t-0.2716\t-0.3293\t-12.8001\t0.5763\t0.7069\n",
      "015100\t-0.1857\t-0.3123\t-13.4514\t0.5964\t0.6856\n",
      "015200\t-0.2892\t-0.2654\t-12.7161\t0.7271\t0.6813\n",
      "015300\t-0.0977\t-0.3347\t-13.5800\t0.7410\t0.7051\n",
      "015400\t-0.2322\t-0.2380\t-13.8086\t0.6843\t0.6979\n",
      "015500\t-0.4334\t-0.3700\t-12.6047\t0.5859\t0.7294\n",
      "015600\t-0.2504\t-0.2523\t-13.5898\t0.7006\t0.7243\n",
      "015700\t-0.1858\t-0.2819\t-12.9952\t0.7191\t0.6991\n",
      "015800\t-0.2558\t-0.4072\t-13.3003\t0.7466\t0.6688\n",
      "015900\t-0.2199\t-0.2080\t-14.0124\t0.6656\t0.6655\n",
      "016000\t-0.1849\t-0.1922\t-13.2492\t0.7783\t0.6958\n",
      "016100\t-0.2633\t-0.4879\t-12.7593\t0.5336\t0.6684\n",
      "016200\t-0.2908\t-0.4954\t-13.1234\t0.6903\t0.6699\n",
      "016300\t-0.1926\t-0.5569\t-13.5687\t0.5811\t0.6818\n",
      "016400\t-0.3990\t-0.2465\t-13.2442\t0.6716\t0.6783\n",
      "016500\t-0.3152\t-0.2220\t-12.8753\t0.7368\t0.7102\n",
      "016600\t-0.3004\t-0.3109\t-13.0976\t0.6499\t0.7346\n",
      "016700\t-0.2115\t-0.3113\t-13.3683\t0.6798\t0.6889\n",
      "016800\t-0.1031\t-0.3748\t-13.2983\t0.7484\t0.6881\n",
      "016900\t-0.4796\t-0.4439\t-13.1747\t0.4709\t0.7038\n",
      "017000\t-0.2105\t-0.2436\t-13.9126\t0.7587\t0.6756\n",
      "017100\t-0.1884\t-0.1929\t-14.4482\t0.6645\t0.6740\n",
      "017200\t-0.2672\t-0.5183\t-12.9649\t0.5930\t0.7265\n",
      "017300\t-0.1782\t-0.3385\t-12.9479\t0.7214\t0.5842\n",
      "017400\t-0.1839\t-0.3069\t-13.4534\t0.7602\t0.6840\n",
      "017500\t-0.1821\t-0.3712\t-14.9464\t0.7662\t0.6979\n",
      "017600\t-0.2109\t-0.4122\t-13.4705\t0.7861\t0.7110\n",
      "017700\t-0.0520\t-0.4411\t-13.9657\t0.5972\t0.5934\n",
      "017800\t-0.3008\t-0.3746\t-13.3126\t0.6654\t0.7088\n",
      "017900\t-0.2512\t-0.2782\t-13.6540\t0.7475\t0.7062\n",
      "018000\t-0.2412\t-0.1529\t-12.6032\t0.5914\t0.6368\n",
      "018100\t-0.3460\t-0.2689\t-14.5198\t0.7246\t0.7237\n",
      "018200\t-0.3468\t-0.3287\t-13.5469\t0.7103\t0.7052\n",
      "018300\t-0.3664\t-0.1177\t-14.0803\t0.7383\t0.7297\n",
      "018400\t-0.1835\t-0.1464\t-13.5029\t0.6483\t0.7001\n",
      "018500\t-0.3542\t-0.1934\t-12.9005\t0.6918\t0.5367\n",
      "018600\t-0.2340\t-0.2807\t-13.2173\t0.7225\t0.6947\n",
      "018700\t-0.1826\t-0.3464\t-14.1123\t0.6525\t0.6512\n",
      "018800\t-0.2719\t-0.2150\t-13.9886\t0.6816\t0.6146\n",
      "018900\t-0.1927\t-0.0306\t-14.0045\t0.7736\t0.7319\n",
      "019000\t-0.3439\t-0.3858\t-13.6323\t0.6509\t0.7107\n",
      "019100\t-0.1685\t-0.3183\t-13.7778\t0.7384\t0.7194\n"
     ]
    }
   ],
   "source": [
    "folder = 'cyclegan_48_wgan_1_val2'\n",
    "path = 'results/'+folder\n",
    "path_imgs = path +'/samples'\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "if not os.path.exists(path_imgs):\n",
    "    os.mkdir(path_imgs)\n",
    "if not os.path.exists(path_imgs+'/A'):\n",
    "    os.mkdir(path_imgs+'/A')\n",
    "if not os.path.exists(path_imgs+'/B'):\n",
    "    os.mkdir(path_imgs+'/B')\n",
    "    \n",
    "print(\n",
    "    f\" Iter.\\t\"\n",
    "    f\"LossD A\\t\"\n",
    "    f\"LossD B\\t\"\n",
    "    f\" Loss G\\t\"\n",
    "    f\"Acc B2A\\t\"\n",
    "    f\"Acc A2B\")\n",
    "\n",
    "    \n",
    "best_mean_acc = 0\n",
    "early_stop_cnt = 0\n",
    "\n",
    "iter_A = iter(trainloader_A)\n",
    "iter_B = iter(trainloader_B)\n",
    "\n",
    "for i in range(1, 200001):\n",
    "    try:\n",
    "        data_A = next(iter_A)\n",
    "    except:\n",
    "        iter_A = iter(trainloader_A)\n",
    "        data_A = next(iter_A)\n",
    "\n",
    "    try:\n",
    "        data_B = next(iter_B)\n",
    "    except:\n",
    "        iter_B = iter(trainloader_B)\n",
    "        data_B = next(iter_B)\n",
    "\n",
    "    # get batch size data\n",
    "    real_image_A1 = data_A.to(device1)\n",
    "    real_image_B1 = data_B.to(device1)\n",
    "    real_image_A2 = data_A.to(device2)\n",
    "    real_image_B2 = data_B.to(device2)\n",
    "\n",
    "    batch_size = real_image_A1.size(0)\n",
    "\n",
    "    # real data label is 1, fake data label is 0.\n",
    "    real_label1 = torch.full((batch_size, 1), 1, device=device1, dtype=torch.float32)\n",
    "    fake_label1 = torch.full((batch_size, 1), 0, device=device1, dtype=torch.float32)\n",
    "    real_label2 = torch.full((batch_size, 1), 1, device=device2, dtype=torch.float32)\n",
    "    fake_label2 = torch.full((batch_size, 1), 0, device=device2, dtype=torch.float32)\n",
    "\n",
    "    ##############################################\n",
    "    # (1) Update G network: Generators A2B and B2A\n",
    "    ##############################################\n",
    "\n",
    "    # Set G_A and G_B's gradients to zero\n",
    "    optimizer_G.zero_grad()\n",
    "\n",
    "    # Identity loss\n",
    "    # G_B2A(A) should equal A if real A is fed\n",
    "    identity_image_A = netG_B2A(real_image_A1)\n",
    "    loss_identity_A = identity_loss1(identity_image_A, real_image_A1) * 5.0\n",
    "    # G_A2B(B) should equal B if real B is fed\n",
    "    identity_image_B = netG_A2B(real_image_B2)\n",
    "    loss_identity_B = identity_loss2(identity_image_B, real_image_B2) * 5.0\n",
    "\n",
    "    # GAN loss D_A(G_A(A))\n",
    "    fake_image_A = netG_B2A(real_image_B1)\n",
    "    fake_output_A = netD_A(fake_image_A)\n",
    "#         loss_GAN_B2A = adversarial_loss1(fake_output_A, real_label1)\n",
    "    # WGAN\n",
    "    loss_GAN_B2A = -fake_output_A.mean()\n",
    "\n",
    "    # GAN loss D_B(G_B(B))\n",
    "    fake_image_B = netG_A2B(real_image_A2)\n",
    "    fake_output_B = netD_B(fake_image_B)\n",
    "#         loss_GAN_A2B = adversarial_loss2(fake_output_B, real_label2)\n",
    "    # WGAN\n",
    "    loss_GAN_A2B = -fake_output_B.mean()\n",
    "\n",
    "    # Cycle loss\n",
    "    recovered_image_A = netG_B2A(fake_image_B.to(device1))\n",
    "    loss_cycle_ABA = cycle_loss1(recovered_image_A, real_image_A1) * 10.0\n",
    "\n",
    "    recovered_image_B = netG_A2B(fake_image_A.to(device2))\n",
    "    loss_cycle_BAB = cycle_loss2(recovered_image_B, real_image_B2) * 10.0\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    # Combined loss and calculate gradients\n",
    "    errG = loss_identity_A.cpu() + loss_identity_B.cpu() + loss_GAN_A2B.cpu() + loss_GAN_B2A.cpu() + loss_cycle_ABA.cpu() + loss_cycle_BAB.cpu()\n",
    "\n",
    "    g_losses.append(errG.item())\n",
    "    # Calculate gradients for G_A and G_B\n",
    "    errG.backward()\n",
    "    # Update G_A and G_B's weights\n",
    "    optimizer_G.step()\n",
    "\n",
    "    ##############################################\n",
    "    # (2) Update D network: Discriminator A\n",
    "    ##############################################\n",
    "\n",
    "    # Set D_A gradients to zero\n",
    "    optimizer_D_A.zero_grad()\n",
    "\n",
    "    # Real A image loss\n",
    "    real_output_A = netD_A(real_image_A1)\n",
    "#         errD_real_A = adversarial_loss1(real_output_A, real_label1)\n",
    "    # WGAN\n",
    "    errD_real_A = real_output_A.mean()\n",
    "\n",
    "    # Fake A image loss\n",
    "    fake_image_A = fake_A_buffer.push_and_pop(fake_image_A)\n",
    "    fake_output_A = netD_A(fake_image_A.detach().to(device1))\n",
    "#         errD_fake_A = adversarial_loss1(fake_output_A, fake_label1)\n",
    "    # WGAN\n",
    "    errD_fake_A = fake_output_A.mean()\n",
    "\n",
    "    # Combined loss and calculate gradients\n",
    "#         errD_A = (errD_real_A + errD_fake_A) / 2\n",
    "    # WGAN\n",
    "    gp_A = gradient_penalty(netD_A, real_image_A1, netG_B2A(real_image_B1), device1)\n",
    "    errD_A = (errD_fake_A - errD_real_A + gp_A)\n",
    "\n",
    "    d_losses_A.append(errD_A.item())\n",
    "\n",
    "    # Calculate gradients for D_A\n",
    "    errD_A.backward()\n",
    "    # Update D_A weights\n",
    "    optimizer_D_A.step()\n",
    "\n",
    "    ##############################################\n",
    "    # (3) Update D network: Discriminator B\n",
    "    ##############################################\n",
    "\n",
    "    # Set D_B gradients to zero\n",
    "    optimizer_D_B.zero_grad()\n",
    "\n",
    "    # Real B image loss\n",
    "    real_output_B = netD_B(real_image_B2)\n",
    "#         errD_real_B = adversarial_loss2(real_output_B, real_label2)\n",
    "    # WGAN\n",
    "    errD_real_B = real_output_B.mean()\n",
    "\n",
    "    # Fake B image loss\n",
    "    fake_image_B = fake_B_buffer.push_and_pop(fake_image_B)\n",
    "    fake_output_B = netD_B(fake_image_B.detach().to(device2))\n",
    "#         errD_fake_B = adversarial_loss2(fake_output_B, fake_label2)\n",
    "    # WGAN\n",
    "    errD_fake_B = fake_output_B.mean()\n",
    "\n",
    "    # Combined loss and calculate gradients\n",
    "#         errD_B = (errD_real_B + errD_fake_B) / 2\n",
    "    # WGAN\n",
    "    gp_B = gradient_penalty(netD_B, real_image_B2, netG_A2B(real_image_A2), device2)\n",
    "    errD_B = (errD_fake_B - errD_real_B + gp_B)\n",
    "\n",
    "    d_losses_B.append(errD_B.item())\n",
    "\n",
    "    # Calculate gradients for D_B\n",
    "    errD_B.backward()\n",
    "    # Update D_B weights\n",
    "    optimizer_D_B.step()\n",
    "        \n",
    "#     torch.cuda.synchronize()\n",
    "    if i%100 == 0:\n",
    "        accA = eval_G(netG_B2A, ClfA, testloader1, device1, device3)\n",
    "        accB = eval_G(netG_A2B, ClfB, testloader0, device2, device4)\n",
    "        \n",
    "        acc_a.append(accA)\n",
    "        acc_b.append(accB)\n",
    "\n",
    "        print(\n",
    "            f\"{i:06d}\\t\"\n",
    "            f\"{d_losses_A[-1]:2.4f}\\t\"\n",
    "            f\"{d_losses_B[-1]:2.4f}\\t\"\n",
    "            f\"{g_losses[-1]:2.4f}\\t\"\n",
    "            f\"{accA:2.4f}\\t\"\n",
    "            f\"{accB:2.4f}\")\n",
    "    \n",
    "        mean_acc = accA+accB\n",
    "        if mean_acc < best_mean_acc:\n",
    "            early_stop_cnt+= 1\n",
    "        else:\n",
    "            early_stop_cnt = 0\n",
    "            best_mean_acc = mean_acc\n",
    "            # save last check pointing\n",
    "            torch.save(netG_A2B.state_dict(), path+\"/netG_A2B.pth\")\n",
    "            torch.save(netG_B2A.state_dict(), path+\"/netG_B2A.pth\")\n",
    "            torch.save(netD_A.state_dict(), path+\"/netD_A.pth\")\n",
    "            torch.save(netD_B.state_dict(), path+\"/netD_B.pth\")\n",
    "\n",
    "#         if early_stop_cnt > 19:\n",
    "#             break\n",
    "    \n",
    "    if i%1000 == 0:\n",
    "        fake_image_A = 0.5 * (netG_B2A(real_image_B1).data[:16] + 1.0)\n",
    "        fake_image_B = 0.5 * (netG_A2B(real_image_A2).data[:16] + 1.0)\n",
    "\n",
    "        vutils.save_image(fake_image_A.detach(),\n",
    "                        path_imgs+f\"/A/{i:06d}_fake.png\",\n",
    "                        normalize=True)\n",
    "        vutils.save_image(fake_image_B.detach(),\n",
    "                        path_imgs+f\"/B/{i:06d}_fake.png\",\n",
    "                        normalize=True)\n",
    "\n",
    "    if i%500 == 0:\n",
    "        # Update learning rates\n",
    "        lr_scheduler_G.step()\n",
    "        lr_scheduler_D_A.step()\n",
    "        lr_scheduler_D_B.step()\n",
    "\n",
    "torch.save(netG_A2B.state_dict(), path+\"/netG_A2B_last.pth\")\n",
    "torch.save(netG_B2A.state_dict(), path+\"/netG_B2A_last.pth\")\n",
    "torch.save(netD_A.state_dict(), path+\"/netD_A_last.pth\")\n",
    "torch.save(netD_B.state_dict(), path+\"/netD_B_last.pth\")\n",
    "\n",
    "np.save(path+'/d_losses_A.npy', d_losses_A)\n",
    "np.save(path+'/d_losses_B.npy', d_losses_B)\n",
    "np.save(path+'/g_losses.npy', g_losses)\n",
    "np.save(path+'/acc_a.npy', acc_a)\n",
    "np.save(path+'/acc_b.npy', acc_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-80906f6525eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print('Acc for A:', eval_Clf(ClfA, testloader0, device3))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print('Acc for B:', eval_Clf(ClfB, testloader1, device4))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(acc_a)\n",
    "plt.plot(acc_b)\n",
    "# print('Acc for A:', eval_Clf(ClfA, testloader0, device3))\n",
    "# print('Acc for B:', eval_Clf(ClfB, testloader1, device4))\n",
    "max(acc_a), max(acc_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ClfA.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6978389024734497, 0.6619467735290527)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_a[-1], acc_b[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netG_A2B.state_dict(), path+\"/netG_A2B.pth\")\n",
    "torch.save(netG_B2A.state_dict(), path+\"/netG_B2A.pth\")\n",
    "torch.save(netD_A.state_dict(), path+\"/netD_A.pth\")\n",
    "torch.save(netD_B.state_dict(), path+\"/netD_B.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 48, 48]           3,136\n",
      "         LeakyReLU-2           [-1, 64, 48, 48]               0\n",
      "            Conv2d-3          [-1, 128, 24, 24]         131,200\n",
      "    InstanceNorm2d-4          [-1, 128, 24, 24]               0\n",
      "         LeakyReLU-5          [-1, 128, 24, 24]               0\n",
      "            Conv2d-6          [-1, 256, 12, 12]         524,544\n",
      "    InstanceNorm2d-7          [-1, 256, 12, 12]               0\n",
      "         LeakyReLU-8          [-1, 256, 12, 12]               0\n",
      "            Conv2d-9          [-1, 512, 11, 11]       2,097,664\n",
      "   InstanceNorm2d-10          [-1, 512, 11, 11]               0\n",
      "        LeakyReLU-11          [-1, 512, 11, 11]               0\n",
      "           Conv2d-12            [-1, 1, 10, 10]           8,193\n",
      "================================================================\n",
      "Total params: 2,764,737\n",
      "Trainable params: 2,764,737\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 6.20\n",
      "Params size (MB): 10.55\n",
      "Estimated Total Size (MB): 16.85\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(netD_A, (3, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f37753e0358>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnXl03NWR77+lVkvd2ndbsmRL8r5gbKOw7xAgmYRlJiFkQgJ5DCaTkEkyZA6EeROYeZNzIC9sJ49J4gwMyxAIBAjwwiRsydgkYZGN9x1btixrs7VvLam73h/dnjHK/baFZbfMu/U5x8et++3q3/3d7upf962uKlFVGIbhH2mTPQHDMCYHc37D8BRzfsPwFHN+w/AUc37D8BRzfsPwFHN+w/AUc37D8BRzfsPwlPSJGIvIZQAeABAA8K+qeley+xfmFGhFcblTC4fD1G54eMg5npERpDa9fT18IhKjUjAzk2qhvDy3kORHktHYKNeiI1TLCPF5tDY3U62kqMQ5nib8qRbl14Co8Dm2tbRRLUCOFxvhi3Wws4NqmYEQ1SQtwDUh5ybUBIORfqrV1s6gWmaIr3FPdxfVMoLu57q3h88jMuR+XnojvRgcHUpydv/NUTu/iAQAPAjg4wD2AXhXRF5U1c3MpqK4HE/e+ohTW7xkIT1WY8NO5/i0GaXUZuXvX6NaNOB+MwGAylk1VJt70cVugb+XoKeXv6A7uluoVj23mmr3383fY6/73Jed49lBvlaBKH/j7UvjbzQP3P0g1QqD7jeh/uYItXn8mSeoNruAvz7SM3O5FnKfWyzI34Q2b3+Xak99/8dUmznLfc4A8MrLL1CtqmKWc3zla/XUZsfWRuf4M5v5ccYykY/9pwLYqaq7VHUYwFMArpjA4xmGkUIm4vzTABz+9rMvMWYYxkeA477hJyLLRaReROo7+/j3HsMwUstEnL8JQNVhf1cmxj6Aqq5Q1TpVrSvMKZjA4QzDOJZMxPnfBTBbRGpEJAPANQBePDbTMgzjeCMTKeYhIp8EcD/iob6HVfV7ye5fHC7SS6vdO+bdnQepXXqaO3LxuauvpDZ/edN1VNu6ju/m7m3+kw8v/8W0mbOd41Wz3Lu1AJBXy3eAowO9VIuM8K9IjY17qDa3doFbGMmmNv2dPJyXPSuHaht+v5pqBxvdUY5wlIfstm3cRrVXXnmDagNDw1SbUunehtrXxp/nBYtqqXb3Q9/n82jcRbWRQR7lGOl3hyq//IVvUJuAusPOv2t5FV2RjuMb6gMAVX0ZwMsTeQzDMCYH+4WfYXiKOb9heIo5v2F4ijm/YXiKOb9heMqEdvs/LFHEMBBwh2UyivOp3VCfO7vpmZd4oOGU086g2ob1DVQrK59Kte99917neESj1Oamv1lOtUuuuZRqYeHZgHMXLqba+rfXOMdDGUXUZs5J/PFGRnko+KSzTqEaBsg4T8REzWL+6/BF57nDrABwsIcnTzUfbHc/3pK/pDaz5/JQHzIHqTSQzhPGSqbx11Xzzm7neE4hX4+OVnfoUHVcUT4AduU3DG8x5zcMTzHnNwxPMec3DE8x5zcMT0npbv9odBQtne66b+nCa9Zh2F0nK5hbSE1+88oqql1w7nlUu/t//4Bqre3uZBvN4Mu4dzcv1ZXsrTfKAwjAME8SOUhqFw7285qGZZW8dFnOdF4iq3m/eycdAEJp7jUpLCR1EAHkV2ZRrexkHlmIDbPQAtA50OeeB6l1CAD9SRKu+gf5OecU87VqaeF2gbA71b1rgNfwq5njLmu2ppcnQI3FrvyG4Snm/IbhKeb8huEp5vyG4Snm/IbhKeb8huEpKQ31ZWRkYEZVlVNb8+571G5OjbsuXSRJ7bb9+3mI7dnnfkm1fft5h5pR0uNpYJCHZOYvXkQ18IgdBqNcDOfwsOi5l1zgHO/t422FMjN4nb49e3dQrbiAJwuFA6SFVjqfRyyNJzMla7E2EOEJNcVFZc7xAx2d1CacyesMxkb485KdxdcxNupO3gGA3p4DzvG+AV7Xcve+rc7x4RG+FmOxK79heIo5v2F4ijm/YXiKOb9heIo5v2F4ijm/YXjKhEJ9ItIAoBdAFMCoqtYlu39sNIpIhztj6pQkdekaG9ytlbKzeBZVcRnP+HvzrTeplp5HQlQAOrrcteKmVldSm7Mu59loo8O8TVYwh4ebRtN4yl8k4g47BjJ5GCqSJHxVO53XnotG+DzS08PO8eGDvA1ZOMBbirVs5u21ps6ZS7WeJvfxSgrdIWcAyb1iNEl8Nkn9vOFeXvuvv8et/cWfu1vbAcCzv3DXr4zFePh7LMcizn+BqroDlYZhnLDYx37D8JSJOr8CeEVEVosIr1FtGMYJx0Q/9p+tqk0iUgbgVRHZqqorD79D4k1hOQCE0/n3WMMwUsuErvyq2pT4vw3A8wBOddxnharWqWpdRiBjIoczDOMYctTOLyLZIpJ76DaASwBsPFYTMwzj+DKRj/1TADwvIoce52eq+utkBmkQhEm/pliSsFFhvrvoY3sXDzL83Q9vpVr+PbxQ5A9/8iDVyufOcI5/+/ZvUZtk7an27d9PtdwCbpgT5J+gBvvdxSzzsngYLTcvSVixj2ejpYd4qHVovzsjLVTAQ4e333gz1WbO4u26Kiq2U23pKWc6xwf38/N68ol/p1phAV/7k0/mc5w+3Z1dCADVF3zMOR5K46+B9mZ39unO17ZQm7EctfOr6i4AJx+tvWEYk4uF+gzDU8z5DcNTzPkNw1PM+Q3DU8z5DcNTRFVTdrCqokr91iVfc2pbt7kLEgJA36g7U23Wolpq80/33MEnksuzr3r6k4S2stwhwqzcfGrT3s4LRZaWuHu0AQAkSTHLCO9NFyHZY4EYL/qZnsczILsad1ItLcZDUbkZ7nP71wcfojYD3TzzLZjk16Gvvc77Mm7f2egczyBZhwCQl8/DoqFMHpL+yleu5Y+ZJESYm+/OuMzK4a+rhee7+03WXViH+rX1/AV+GHblNwxPMec3DE8x5zcMTzHnNwxPMec3DE9JabuurKwsLFnmLvNXM4/XYSuZ5m4L1RN119QDAOQmObUwr52Xns7fD0XckZGW1jZqM7WMJ3TEBvjOcdoIr8U22MkjEkM9fc7xwpxSaoMkLZ7e/NXvqVacy89t80Z3sk2kh5/XmXVnUe0f7riTastO+ZNM8v9iZMj9nE0t43UXc3N5wtLuhk1U+8N/8rW6+OKzqTYQc0ezdm1zRyoAYOMad3Sss7Wd2ozFrvyG4Snm/IbhKeb8huEp5vyG4Snm/IbhKeb8huEpKQ317Wvaj1u/8w9OLSOLT2Ug5g5fLT3nJGpz/uU8bJQV4kkuAwM8aQYxd2hu6/pd1KTsNB4OS0vjrcG21vOEmlWv/gfVov3u5JiZldOpzccv/QTVmjbtoVrBPHdtRQDIS3MnQc2cyZOx/u5bf0u1ymp3/UQA2L5zM9WGo+72Ws3te6lNKFxNtVk1XBvo5iHfpoZ9VJu/cIFzfG4tD39D3YlCmZk8YWksduU3DE8x5zcMTzHnNwxPMec3DE8x5zcMTzHnNwxPOWKoT0QeBvApAG2quigxVgTg5wCqATQAuFpVebG6BOFQCItq3WGN9m7eekuG3SXJtmzirYmyS6ZQbaAnSZusTF43LU3ctd3OP7eC2jx8/3NU2/TuWqo9+9zjVPvO175KtVDAXeuuPUm2V9u+JqotqZ1JtcHeHqpVFLgzMV944QVqc/JJC6nWMdhLtfxcdw08AEgLuUNisSi/7rW08fWYWVXO7Vr462rZ4i9QLRRyP2c9fbym4cfOcLf4ygof21DfIwAuGzN2G4DXVXU2gNcTfxuG8RHiiM6vqisBjE2cvwLAo4nbjwK48hjPyzCM48zRfuefoqqH2oS2IN6x1zCMjxAT3vDTeOF/WvxfRJaLSL2I1A9FecUYwzBSy9E6f6uIlANA4n/6o2ZVXaGqdapaxzajDMNIPUfr/C8CuC5x+zoAfAvXMIwTkvGE+p4EcD6AEhHZB+AOAHcBeFpEbgCwB8DV4zlYdHQU3R3uopsa5e2pAqMx5/iCxTw01NXAwy4FtTzTLlnYq6zCHb5673c7qM13vvM/qTajYBrV5lXxc3vu2Zeodv8933OOB9LcawgA6zZtoFr+KC+4WVvBMwU3bW1wz0N50dJolGtDAzzstaeJZ8wVT3GH5gZ6+VfQubWzqbZh3TqqXX/tZ6gWUL7+mQH3NTg/m4ft3ln5hnO8v4+HRMdyROdX1c8T6aJxH8UwjBMO+4WfYXiKOb9heIo5v2F4ijm/YXiKOb9heEpKC3gGg+koLy12ah29PClwsMcdvvj0pz9NbYaHeYgKg1yL/2CR4E4uxK9f+Q01mVI4lWqFBYVU27WbF6X8wl/w855/mrtvXePm9dRGgvxlUJDHw02N+3kvufJy93nn5/Osyf4DvABmSXEJ1Zac4u7/CAC797gLkBaEeT++xgZekPXrX+UZlbWVPIS8acsaqs2fP985HspyZ5ECwE9W/ItzvL2dr+FY7MpvGJ5izm8YnmLObxieYs5vGJ5izm8YnmLObxiektJQHwBA3Nl7LS08M2s46A6/zZw7i9oUTXGHFAGgrbOValNqKqkGknRWM4f3kUvP5NlcLQd4BuH8eXOodtWfX041DLl7Dba383NOJ1llAFAzj2cX7m15i2rTprozFi+7/Cpqs6+ZZ2Jm5bl7/wHA/la+jp3t7sKwFVN4CPbs68+kWiRJSDqmPDN10ULeV3KQZE4+/+yL1KZsurvnYbCZF4Udi135DcNTzPkNw1PM+Q3DU8z5DcNTzPkNw1NSutufliYI57or+NbM422hFp62xDm+4Dz3OABEIryWWThJwsSBHr4rHul379xfcwNPtHnuF7xdV+OmnVSbM7OKarnZ7hZUAPDHN151jpeV8MSYgjDfSf/hv/2cavv28CSS6G/ciSyRIb4jPqWMR2hGYu4oBgCUFOdR7ZwzljnHp0/j9QdjwxGqVZTz5J3RUV4XUANBqoVzCpzjgSye+JUG0qIsMH6Xtiu/YXiKOb9heIo5v2F4ijm/YXiKOb9heIo5v2F4ynjadT0M4FMA2lR1UWLsTgA3AmhP3O12VX35SI8Vygph3mJ3vbL2LncbLwC45a5b3UKScnv9UR4aGgTXApmkUB+AjHx3mLK7v4/aPP3Sv1HthYd/SbUr/uzPqParxx+j2txqd5LRgSRJM6eddyHVNtz3CLerO4tqG9duc45n5fKXXDDMQ7D7dvKWaLHRHqodbNvrHN+UmUltbv/771Jt3ep3qRbK4SHTXQ08ce3iy690jlfUuucOAKuedyf9DA4lqV05hvFc+R8BcJlj/D5VXZL4d0THNwzjxOKIzq+qKwHwy7JhGB9JJvKd/2YRWS8iD4sI/ymSYRgnJEfr/D8CMBPAEgDNAO5hdxSR5SJSLyL1vYP9R3k4wzCONUfl/KraqqpRVY0B+CkAd6eI+H1XqGqdqtblJtnQMQwjtRyV84tI+WF/XgVg47GZjmEYqWI8ob4nAZwPoERE9gG4A8D5IrIE8WBbA4CbxnOwWLogUuo+5Hfu+SduOOKO6UU72p3jABBSHs7LSOcxwrR0njHXF3Vnew2BZ6qFAwGqXbHcHeIBAIxwqayaZ/wd6HZ/tSot4bUJd67lrcEKlK9Hy3aelVgYcodMKyp57by+QR4ynV7L6zWORgep1t/r3qsumsLbdb1Zv5JqJSVFVJs2na9xR4SHI+vfecM5fu3VF1ObrvatzvF1rb+lNmM5ovOr6ucdww+N+wiGYZyQ2C/8DMNTzPkNw1PM+Q3DU8z5DcNTzPkNw1NSWsAzNzsPZ592kVO74fK/onaf+5w7JFZdW+4cB4CqWRVUyynmv0Zu7+AFPDNJ1lZaOs8QSwMv3DjUxX/xGEryg6jpVTzU9+bW37kfr5LPo6eDt6DKDoWpFkgSxiybMsU5PjDAi1xu2LSJagX57iKXAFBTw9ejINdtt2P7Bmpz6aWfoFo0xsPE77y9mmrnX3gB1V58+T+c46OjvJDozV//unP8kVXPU5ux2JXfMDzFnN8wPMWc3zA8xZzfMDzFnN8wPMWc3zA8JaWhvr17GvGNv/q2UwuF+VQefehp53hUeNjoK9+4kWqnX3gG1fJyS6kWS3NnqvX38ayy3CQFK0cHeObhLx9/gmpzq6qplpXlDkcWlfA+eEUlPPRZ9LY7ewwAtmzbTrVozN3XMJSkT2IsSiX09/G1Ghp0HwsARofd6ZE5Ofx5LizgmYetbU1UKyh0hzcBYO16nvWen5/vHP/Zz/6d2nzpy9e7BV5/9k+wK79heIo5v2F4ijm/YXiKOb9heIo5v2F4Skp3+4dHhtHQvMepLa5dRO3ys9071bNPqqU27Qe6qZaRmUO1WBrfOR4edO84B0f4e+j+HQ1UW3Hfg1SrLZ9OtQZS0xAAamvda9JykCcsjYzwgoEXXcwTUgIZPLGn/YC7dt7AAN/SzwnzBKlR/rQgGuWP2d3nTp4qK+WJX9EY3zLv6U1SL7Cfv+ZmzOTP54IF7rmUTi2hNj/5sfu1097eRm3GYld+w/AUc37D8BRzfsPwFHN+w/AUc37D8BRzfsPwlPG066oC8BiAKYi351qhqg+ISBGAnwOoRrxl19WqyovBASjKL8IXzr3GqW3ZzhMf3ln9lnP8jI9/jB8syMM1MfDQUHomD19FOtyJRM273OFLABho76LajddeR7W8DJ4Ak6zmXiTirvu2cSNf3/aDB6lWlTeDarNqa6jW2ekO9e3Z+z61ycnjLbRm186mWmYmDxG+U/+mc/ym5XdTmz17Gqg2YwavF/iHd/ZSbVaQv67WvOeu/RfI4K/hT336Muf4v/zanQTnYjxX/lEAt6jqAgCnA/iaiCwAcBuA11V1NoDXE38bhvER4YjOr6rNqromcbsXwBYA0wBcAeDRxN0eBZCk66RhGCcaH+o7v4hUA1gK4G0AU1S1OSG1IP61wDCMjwjjdn4RyQHwLIBvquoH+g2rqiK+H+CyWy4i9SJSPzDM69QbhpFaxuX8IhJE3PGfUNXnEsOtIlKe0MsBOH9UrKorVLVOVeuykmxiGYaRWo7o/CIiAB4CsEVV7z1MehHAoe3q6wC8cOynZxjG8WI8WX1nAfgigA0isjYxdjuAuwA8LSI3ANgD4OojPVBObhhnnrfYqf2ve3iwYP9Bd920yrk8U2ogymu+jYC3QRru42HAQZK1pSPD1Oa0U06h2rZ6Hn5r6eBho94OHj7MyXPXg9u+dRe1KSsv4/NobqHaSJJ2UgUFec7xutOuoDYdnQeo9sc//pHPI0lW4mWXnOMcD4V5GG1klH89bdzXQLUFC+ZR7cABfm4LFy50jk8/nb92Wtevdwsy/iJ+R3R+VX0TvCygu/GeYRgnPPYLP8PwFHN+w/AUc37D8BRzfsPwFHN+w/CUlBbwDGeHcNLp7uysgRgPhVQudWeW9fdwm2iAF7nMyyug2u6dPEOvs819vC1JWjEtrp1PtQPt7VSrKq2k2ryZPKT0g+/f6xxPT8ugNlkhnk0HHkXD8BAPi/b09DjH9zc1Ups582ZR7TOfvYtqW7ZtplpBfpFzvK+fZ0bOms2zFYPBINW6et3nDABL6+qo1tK8zzm+6qnnqU15hbulWGw0Sc+zMdiV3zA8xZzfMDzFnN8wPMWc3zA8xZzfMDzFnN8wPCWlob6RaARNnbud2vQlvFBkf4+7z1xGNi/cGInxTLuBAd5TLT+fh73a9ux3joczQ9Tm8ccep9qiGh4GjIyMUm3Vm+6CpgBw4cWXOscfeewxatN5kGexLVvEi6RGIrxvXXmFO1QZDPLrzd697pAXAMybP4fbNfDwbPb8LOf4NDI/ANi2lRcZbW7hPQ9FeJHOzZt2UK2wsNA5Xl7O+wk27nHPY3iYv27GYld+w/AUc37D8BRzfsPwFHN+w/AUc37D8JSU7vZnZGZi+pxap6bCM0iGou5acd1dvfxYGTyRJSfHXV8OAEYifLe0pmamc7wgkycKbV27jWqBHF7NuLqaRwIampyFkgEA7d3uNSmr4PUOB/v4bv9IkjyRxiZe328eSdJpa+M7+uEs/pzt3NZAtasu5+Uj33/fXbvwgXt/TG36+3ltwuysHKp1dvPEnsZ9PKGpptqdSFReWU5tPnv1Z53jmZlhajMWu/IbhqeY8xuGp5jzG4anmPMbhqeY8xuGp5jzG4anHDHUJyJVAB5DvAW3Alihqg+IyJ0AbgRwqBDd7ar6crLHCmZmYNp0d8gpMspDfZkhd0gsM8DDLl1dvKVVOBij2pRyHhJr2+MOU809eQl/vCS1+IYG+Dmv283r0mUW8fMOk8SkinaeJHKwjdcS3NnoTsQCgLw8HqrcsdsdYivK52FWKG811dvL1+of77ibam2t7nMLBnkyVkGBu+4fAHR38WSm/NwSqlWfxROTmkhdw6Y9zdRmzbtrnOMD/bxN3VjGE+cfBXCLqq4RkVwAq0Xk1YR2n6r+YNxHMwzjhGE8vfqaATQnbveKyBYA0473xAzDOL58qO/8IlINYCmAtxNDN4vIehF5WETcScmGYZyQjNv5RSQHwLMAvqmqPQB+BGAmgCWIfzK4h9gtF5F6Ealvb+d19g3DSC3jcn4RCSLu+E+o6nMAoKqtqhpV1RiAnwI41WWrqitUtU5V60pL+YaIYRip5YjOLyIC4CEAW1T13sPGD886uAoAb1tjGMYJx3h2+88C8EUAG0RkbWLsdgCfF5EliIf/GgDcdKQH0hgQIS2eYsLfh1iWVXFxMbXJz+GPFxnk9f2yk9jlF5DsvRCv3XZwkIcch0d4+Go4xNPpzj7rHKqlkWzGuct4qGn3dl6zrv/gENVaWnhW35q3VzvHc4t5qC8Y4Fl977y1lmqRQR4iLCt1h27T0/mxDhzgX09LSkupFkhPFqrkGagzZlS5Hy/IH2/X+zud45EIf77GMp7d/jcBuGaRNKZvGMaJjf3CzzA8xZzfMDzFnN8wPMWc3zA8xZzfMDwlpQU8W1tbcd+9P3Rqt9x6C7ULprvbcg0nKbaZncXbbvVFebuu3q5OquXmu0N97/5hFbXp6uVFHc8573yqzc9eRDXwpERgyL0mpdU8LDoQ4+uBSh7GvODKi6hWM9cdvlr1ykpqk5edT7Xefr6O3Z08jFY93V10NVmBVwgPsW3ZtolqVdN5ykswgz/m5i3uUGthEQ+LnnPuWc7x8CqerTgWu/IbhqeY8xuGp5jzG4anmPMbhqeY8xuGp5jzG4aniKqm7GDZoSydV+3u4bZ6w3puSKIkPX08xBOL8XhYbi4vPNl58CDVCknxyUAGD4eNRHnmXjBJuGl4hGcetrfyXn0lBe6CSplhfs4Y4sda9epvqVaQx4s3za919xpsbkiSCfgHd1FKAIgN8tfp6iR2fT3uPoRFRbxIpyQJ9a3f/B7Vli09mWoDEV5Ys7evwzl+0skLqc3SpUud49f+899gc8MOfgKHYVd+w/AUc37D8BRzfsPwFHN+w/AUc37D8BRzfsPwlJRm9RWXFOP667/kFoWHcrp63EUwg5k8gyky5A7xAADS+XteRijJY8bcGXMS4eG8QJCHARv3u3v/AUAoxMOAU6aVUy2dvJ/v3eXunQcAaVG+9mdcdCY/FumhCACRdncYtmqRO9QLAJXlvJ/g048/Q7WSqTwbcHDInQ14oIOv/f7W/VSbVjGVan1DSfpDZvHX1Rf/x187x3NzeWZqd7c7+zQtbfzXc7vyG4anmPMbhqeY8xuGp5jzG4anmPMbhqcccbdfREIAVgLITNz/F6p6h4jUAHgKQDGA1QC+qKo8QwRAWVkpbv76V53a6PAgtevt73OO52UGqU0oh+9Ed5PHA4BAgOdESBpZLuWttTKC7vqDAJCTzXdzs7P5/CXJe3Z7uztJZHSE7+hPn1ZJtQHla5WXzuc/NOpusZYZ4glXkqRm3dnnnU61aX97EtU2vPSGc7wglx+rv58njFVVu2sTAsC+/U3crorbhUnS1a6GvdSmbKr7OUsPJqlNOIbxXPkjAC5U1ZMRb8d9mYicDuBuAPep6iwAnQBuGPdRDcOYdI7o/Brn0Nt/MPFPAVwI4BeJ8UcBXHlcZmgYxnFhXN/5RSSQ6NDbBuBVAO8D6FLVQ7962QeA1y02DOOEY1zOr6pRVV0CoBLAqQDmjfcAIrJcROpFpL49SetjwzBSy4fa7VfVLgC/BXAGgAIRObQDVgnAuduhqitUtU5V60pLSiY0WcMwjh1HdH4RKRWRgsTtMICPA9iC+JvAZxJ3uw7AC8drkoZhHHvGk9hTDuBREQkg/mbxtKr+XxHZDOApEflnAO8BeOhIDzQ6OooDB9z158JJQi9TK6Y4xzt7eRgqHM6hWl8PD+VkJQnNBcQd0hsecoe1AEDAE3tysnhCyvAITxbq7OQJJFmZ7vMuncUTUroP8LqFaTn8JTI6zM87K98dBowOcpu33nCH5QCgY3871f7P/fdTbZA819lh/jxf9dnPUC2QxGPKy92vUwDYumM71X710m+c41+67jpqs/I/f+8c70viE2M5ovOr6noAf1ItUFV3If793zCMjyD2Cz/D8BRzfsPwFHN+w/AUc37D8BRzfsPwlJS26xKRdgB7En+WADgRfvJn8/ggNo8P8lGbxwxVLR3PA6bU+T9wYJF6Va2blIPbPGweNg/72G8YvmLObxieMpnOv2ISj304No8PYvP4IP/fzmPSvvMbhjG52Md+w/CUSXF+EblMRLaJyE4RuW0y5pCYR4OIbBCRtSJSn8LjPiwibSKy8bCxIhF5VUR2JP4vnKR53CkiTYk1WSsin0zBPKpE5LcisllENonINxLjKV2TJPNI6ZqISEhE3hGRdYl5/GNivEZE3k74zc9FZPzVOl2oakr/AQggXgasFkAGgHUAFqR6Hom5NAAomYTjngtgGYCNh419H8Btidu3Abh7kuZxJ4Bvp3g9ygEsS9zOBbAdwIJUr0mSeaR0TQAIgJzE7SCAtwGcDuBpANckxn8M4K8ncpzJuPKfCmCnqu7SeKnvpwBcMQnzmDRUdSWAsTW2r0C8ECqQooKoZB4pR1WbVXVN4nYNZvhqAAAB0klEQVQv4sVipiHFa5JkHilF4xz3ormT4fzTADQe9vdkFv9UAK+IyGoRWT5JczjEFFVtTtxuAcArQxx/bhaR9YmvBcf968fhiEg14vUj3sYkrsmYeQApXpNUFM31fcPvbFVdBuATAL4mIudO9oSA+Ds/4m9Mk8GPAMxEvEdDM4B7UnVgEckB8CyAb6rqB3prp3JNHPNI+ZroBIrmjpfJcP4mAIe3L6HFP483qtqU+L8NwPOY3MpErSJSDgCJ/931zo4zqtqaeOHFAPwUKVoTEQki7nBPqOpzieGUr4lrHpO1Joljf+iiueNlMpz/XQCzEzuXGQCuAfBiqichItkiknvoNoBLAGxMbnVceRHxQqjAJBZEPeRsCa5CCtZERATxGpBbVPXew6SUrgmbR6rXJGVFc1O1gzlmN/OTiO+kvg/g7ydpDrWIRxrWAdiUynkAeBLxj48jiH93uwHxnoevA9gB4DUARZM0j8cBbACwHnHnK0/BPM5G/CP9egBrE/8+meo1STKPlK4JgMWIF8Vdj/gbzXcPe82+A2AngGcAZE7kOPYLP8PwFN83/AzDW8z5DcNTzPkNw1PM+Q3DU8z5DcNTzPkNw1PM+Q3DU8z5DcNT/h8jboHYlEoMVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(.5+.5*np.transpose(x_train_0[90], (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 32, 32])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_img = netG_A2B(x_train_0[90:92].to(device))\n",
    "plt.imshow(.5+.5*np.transpose(out_img.cpu().detach()[0], (1, 2, 0)))\n",
    "out_img.shape\n",
    "\n",
    "out_img1 = netG_B2A(out_img)\n",
    "plt.imshow(.5+.5*np.transpose(out_img1.cpu().detach()[0], (1, 2, 0)))\n",
    "out_img.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
