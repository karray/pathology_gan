{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID' \n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import h5py\n",
    "\n",
    "# import torch.backends.cudnn as cudnn\n",
    "# cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        x = F.avg_pool2d(x, x.size()[2:])\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Initial convolution block\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(3, 64, 7),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Downsampling\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Residual blocks\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "\n",
    "            # Upsampling\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Output layer\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, 3, 7),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.res = nn.Sequential(nn.ReflectionPad2d(1),\n",
    "                                 nn.Conv2d(in_channels, in_channels, 3),\n",
    "                                 nn.InstanceNorm2d(in_channels),\n",
    "                                 nn.ReLU(inplace=True),\n",
    "                                 nn.ReflectionPad2d(1),\n",
    "                                 nn.Conv2d(in_channels, in_channels, 3),\n",
    "                                 nn.InstanceNorm2d(in_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.res(x)\n",
    "    \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "        \n",
    "class DecayLR:\n",
    "    def __init__(self, epochs, offset, decay_epochs):\n",
    "        epoch_flag = epochs - decay_epochs\n",
    "        assert (epoch_flag > 0), \"Decay must start before the training session ends!\"\n",
    "        self.epochs = epochs\n",
    "        self.offset = offset\n",
    "        self.decay_epochs = decay_epochs\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_epochs) / (\n",
    "                self.epochs - self.decay_epochs)\n",
    "    \n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        assert (max_size > 0), \"Empty buffer or trying to create a black hole. Be careful.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return torch.cat(to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    x_train = (h5py.File('camelyonpatch_level_2_split_train_x.h5', 'r')['x'][:, 32:64,32:64] - 127.5) / 127.5\n",
    "#     y_train = h5py.File('camelyonpatch_level_2_split_train_y.h5', 'r')['y'][:].reshape(-1,1)\n",
    "#     x_test = (h5py.File('camelyonpatch_level_2_split_test_x.h5', 'r')['x'][:, 32:64,32:64] - 127.5) / 127.5\n",
    "#     y_test = h5py.File('camelyonpatch_level_2_split_test_y.h5', 'r')['y'][:].reshape(-1,1)\n",
    "#     x_valid = (h5py.File('camelyonpatch_level_2_split_valid_x.h5', 'r')['x'][:, 32:64,32:64] - 127.5) / 127.5\n",
    "#     y_valid = h5py.File('camelyonpatch_level_2_split_valid_y.h5', 'r')['y'][:].reshape(-1,1)\n",
    "\n",
    "    wsi = np.load('wsi.npy')\n",
    "\n",
    "    x_train = np.moveaxis(x_train.astype(np.float32), -1, 1)\n",
    "    x_train_0 = torch.from_numpy(x_train[wsi==0])\n",
    "    x_train_1 = torch.from_numpy(x_train[wsi==1])\n",
    "\n",
    "    return x_train_0, x_train_1  #, y_train, x_test, y_test, x_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_0, x_train_1 = load_data()\n",
    "\n",
    "n = min(x_train_0.shape[0], x_train_1.shape[0])\n",
    "\n",
    "trainloader = DataLoader(TensorDataset(x_train_0[:n], x_train_1[:n]), batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
    "# validloader = DataLoader(TensorDataset(x_valid, y_valid), batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG_A2B = Generator().to(device)\n",
    "netG_B2A = Generator().to(device)\n",
    "netD_A = Discriminator().to(device)\n",
    "netD_B = Discriminator().to(device)\n",
    "\n",
    "netG_A2B.apply(weights_init)\n",
    "netG_B2A.apply(weights_init)\n",
    "netD_A.apply(weights_init)\n",
    "netD_B.apply(weights_init)\n",
    "\n",
    "cycle_loss = torch.nn.L1Loss().to(device)\n",
    "identity_loss = torch.nn.L1Loss().to(device)\n",
    "adversarial_loss = torch.nn.MSELoss().to(device)\n",
    "\n",
    "\n",
    "lr = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "# itertools.chain takes a series of iterables and return them as one long iterable.\n",
    "optimizer_G = Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()), lr=lr, betas=betas)\n",
    "optimizer_D_A = Adam(netD_A.parameters(), lr=lr, betas=betas)\n",
    "optimizer_D_B = Adam(netD_B.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "epochs = 500\n",
    "decay_epochs = 100\n",
    "lr_lambda = DecayLR(epochs, 0, decay_epochs).step\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lr_lambda)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=lr_lambda)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=lr_lambda)\n",
    "\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "identity_losses = []\n",
    "gan_losses = []\n",
    "cycle_losses = []\n",
    "\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    progress_bar = tqdm(enumerate(trainloader), total=len(trainloader))\n",
    "    for i, data in progress_bar:\n",
    "        # get batch size data\n",
    "        real_image_A = data[0].to(device)\n",
    "        real_image_B = data[1].to(device)\n",
    "        batch_size = real_image_A.size(0)\n",
    "\n",
    "        # real data label is 1, fake data label is 0.\n",
    "        real_label = torch.full((batch_size, 1), 1, device=device, dtype=torch.float32)\n",
    "        fake_label = torch.full((batch_size, 1), 0, device=device, dtype=torch.float32)\n",
    "\n",
    "        ##############################################\n",
    "        # (1) Update G network: Generators A2B and B2A\n",
    "        ##############################################\n",
    "\n",
    "        # Set G_A and G_B's gradients to zero\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identity loss\n",
    "        # G_B2A(A) should equal A if real A is fed\n",
    "        identity_image_A = netG_B2A(real_image_A)\n",
    "        loss_identity_A = identity_loss(identity_image_A, real_image_A) * 5.0\n",
    "        # G_A2B(B) should equal B if real B is fed\n",
    "        identity_image_B = netG_A2B(real_image_B)\n",
    "        loss_identity_B = identity_loss(identity_image_B, real_image_B) * 5.0\n",
    "\n",
    "        # GAN loss\n",
    "        # GAN loss D_A(G_A(A))\n",
    "        fake_image_A = netG_B2A(real_image_B)\n",
    "        fake_output_A = netD_A(fake_image_A)\n",
    "        loss_GAN_B2A = adversarial_loss(fake_output_A, real_label)\n",
    "        # GAN loss D_B(G_B(B))\n",
    "        fake_image_B = netG_A2B(real_image_A)\n",
    "        fake_output_B = netD_B(fake_image_B)\n",
    "        loss_GAN_A2B = adversarial_loss(fake_output_B, real_label)\n",
    "\n",
    "        # Cycle loss\n",
    "        recovered_image_A = netG_B2A(fake_image_B)\n",
    "        loss_cycle_ABA = cycle_loss(recovered_image_A, real_image_A) * 10.0\n",
    "\n",
    "        recovered_image_B = netG_A2B(fake_image_A)\n",
    "        loss_cycle_BAB = cycle_loss(recovered_image_B, real_image_B) * 10.0\n",
    "\n",
    "        # Combined loss and calculate gradients\n",
    "        errG = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
    "\n",
    "        # Calculate gradients for G_A and G_B\n",
    "        errG.backward()\n",
    "        # Update G_A and G_B's weights\n",
    "        optimizer_G.step()\n",
    "\n",
    "        ##############################################\n",
    "        # (2) Update D network: Discriminator A\n",
    "        ##############################################\n",
    "\n",
    "        # Set D_A gradients to zero\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Real A image loss\n",
    "        real_output_A = netD_A(real_image_A)\n",
    "        errD_real_A = adversarial_loss(real_output_A, real_label)\n",
    "\n",
    "        # Fake A image loss\n",
    "        fake_image_A = fake_A_buffer.push_and_pop(fake_image_A)\n",
    "        fake_output_A = netD_A(fake_image_A.detach())\n",
    "        errD_fake_A = adversarial_loss(fake_output_A, fake_label)\n",
    "\n",
    "        # Combined loss and calculate gradients\n",
    "        errD_A = (errD_real_A + errD_fake_A) / 2\n",
    "\n",
    "        # Calculate gradients for D_A\n",
    "        errD_A.backward()\n",
    "        # Update D_A weights\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        ##############################################\n",
    "        # (3) Update D network: Discriminator B\n",
    "        ##############################################\n",
    "\n",
    "        # Set D_B gradients to zero\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real B image loss\n",
    "        real_output_B = netD_B(real_image_B)\n",
    "        errD_real_B = adversarial_loss(real_output_B, real_label)\n",
    "\n",
    "        # Fake B image loss\n",
    "        fake_image_B = fake_B_buffer.push_and_pop(fake_image_B)\n",
    "        fake_output_B = netD_B(fake_image_B.detach())\n",
    "        errD_fake_B = adversarial_loss(fake_output_B, fake_label)\n",
    "\n",
    "        # Combined loss and calculate gradients\n",
    "        errD_B = (errD_real_B + errD_fake_B) / 2\n",
    "\n",
    "        # Calculate gradients for D_B\n",
    "        errD_B.backward()\n",
    "        # Update D_B weights\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "        progress_bar.set_description(\n",
    "            f\"[{epoch}/{args.epochs - 1}][{i}/{len(dataloader) - 1}] \"\n",
    "            f\"Loss_D: {(errD_A + errD_B).item():.4f} \"\n",
    "            f\"Loss_G: {errG.item():.4f} \"\n",
    "            f\"Loss_G_identity: {(loss_identity_A + loss_identity_B).item():.4f} \"\n",
    "            f\"loss_G_GAN: {(loss_GAN_A2B + loss_GAN_B2A).item():.4f} \"\n",
    "            f\"loss_G_cycle: {(loss_cycle_ABA + loss_cycle_BAB).item():.4f}\")\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            vutils.save_image(real_image_A,\n",
    "                              f\"{args.outf}/{args.dataset}/A/real_samples.png\",\n",
    "                              normalize=True)\n",
    "            vutils.save_image(real_image_B,\n",
    "                              f\"{args.outf}/{args.dataset}/B/real_samples.png\",\n",
    "                              normalize=True)\n",
    "\n",
    "            fake_image_A = 0.5 * (netG_B2A(real_image_B).data + 1.0)\n",
    "            fake_image_B = 0.5 * (netG_A2B(real_image_A).data + 1.0)\n",
    "\n",
    "            vutils.save_image(fake_image_A.detach(),\n",
    "                              f\"{args.outf}/{args.dataset}/A/fake_samples_epoch_{epoch}.png\",\n",
    "                              normalize=True)\n",
    "            vutils.save_image(fake_image_B.detach(),\n",
    "                              f\"{args.outf}/{args.dataset}/B/fake_samples_epoch_{epoch}.png\",\n",
    "                              normalize=True)\n",
    "\n",
    "    # do check pointing\n",
    "    torch.save(netG_A2B.state_dict(), f\"weights/{args.dataset}/netG_A2B_epoch_{epoch}.pth\")\n",
    "    torch.save(netG_B2A.state_dict(), f\"weights/{args.dataset}/netG_B2A_epoch_{epoch}.pth\")\n",
    "    torch.save(netD_A.state_dict(), f\"weights/{args.dataset}/netD_A_epoch_{epoch}.pth\")\n",
    "    torch.save(netD_B.state_dict(), f\"weights/{args.dataset}/netD_B_epoch_{epoch}.pth\")\n",
    "\n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "\n",
    "# save last check pointing\n",
    "torch.save(netG_A2B.state_dict(), f\"weights/{args.dataset}/netG_A2B.pth\")\n",
    "torch.save(netG_B2A.state_dict(), f\"weights/{args.dataset}/netG_B2A.pth\")\n",
    "torch.save(netD_A.state_dict(), f\"weights/{args.dataset}/netD_A.pth\")\n",
    "torch.save(netD_B.state_dict(), f\"weights/{args.dataset}/netD_B.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
