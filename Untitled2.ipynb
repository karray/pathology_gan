{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID' \n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4'\n",
    "\n",
    "# from tqdm import tnrange, tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from skimage.transform import resize\n",
    "# from skimage import data, color\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.activations import relu\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout,BatchNormalization,Activation,PReLU,LeakyReLU, Add, Lambda\n",
    "from keras.layers import UpSampling2D, Conv2D, MaxPooling2D, Conv2DTranspose, GlobalAveragePooling2D, ZeroPadding2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "# from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "from keras import initializers\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam,RMSprop\n",
    "\n",
    "from keras.utils import multi_gpu_model\n",
    "# from keras.utils import HDF5Matrix\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# from tensorflow.keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    x_train = (h5py.File('camelyonpatch_level_2_split_train_x.h5', 'r')['x'][:, 16:80,16:80] - 127.5) / 127.5\n",
    "    y_train = h5py.File('camelyonpatch_level_2_split_train_y.h5', 'r')['y'][:].reshape(-1,1)\n",
    "    x_test = (h5py.File('camelyonpatch_level_2_split_test_x.h5', 'r')['x'][:, 16:80,16:80] - 127.5) / 127.5\n",
    "    y_test = h5py.File('camelyonpatch_level_2_split_test_y.h5', 'r')['y'][:].reshape(-1,1)\n",
    "    x_valid = (h5py.File('camelyonpatch_level_2_split_valid_x.h5', 'r')['x'][:, 16:80,16:80] - 127.5) / 127.5\n",
    "    y_valid = h5py.File('camelyonpatch_level_2_split_valid_y.h5', 'r')['y'][:].reshape(-1,1)\n",
    "              \n",
    "    return x_train, y_train, x_test, y_test, x_valid, y_valid\n",
    "\n",
    "def plot_samples(samples, folder=None, epoch=None):\n",
    "    rt = int(np.sqrt(samples.shape[0]))\n",
    "    r, c = rt, rt\n",
    "    generatedImage = 0.5 * samples + 0.5\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "    axs = [fig.add_subplot(r,c,i+1) for i in range(r*c)]\n",
    "    cnt = 0\n",
    "    for ax in axs:\n",
    "        ax.imshow(generatedImage[cnt],interpolation='nearest')\n",
    "        ax.axis('off')\n",
    "        ax.set_aspect('equal')\n",
    "        cnt+=1\n",
    "    fig.subplots_adjust(wspace=.008, hspace=.03)\n",
    "\n",
    "    if folder:\n",
    "        path = 'results/'+folder+'/samples'\n",
    "        if not os.path.exists('results'):\n",
    "            os.mkdir('results')\n",
    "        if not os.path.exists('results/'+folder):\n",
    "            os.mkdir('results/'+folder)\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        fig.savefig(path+'/epoch_%d.png' % epoch)\n",
    "        plt.close()\n",
    "\n",
    "def wasserstein_loss(y_true,y_pred):\n",
    "    return K.mean(y_true*y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2621\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test, x_valid, y_valid = load_data()\n",
    "\n",
    "percent = int(x_train.shape[0]*.01)\n",
    "np.random.seed(17)\n",
    "idx_small = np.random.choice(range(x_train.shape[0]), percent, replace=False)\n",
    "\n",
    "x_train_small = x_train[idx_small]\n",
    "y_train_small = y_train[idx_small]\n",
    "plt.hist(y_train_small, bins=2)\n",
    "print(percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/semi-supervised-generative-adversarial-network/\n",
    "# https://github.com/cympfh/GAN-semisup-MNIST-Keras\n",
    "\n",
    "class ImprovedSGAN:\n",
    "    def __init__(self, width, height, channels, latent_dim=100):\n",
    "        self.width=width\n",
    "        self.height=height\n",
    "        self.channels=channels\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.classifier, self.discriminator, self.generator, self.gan = self.build()\n",
    "#         self.discriminator, self.classifier = define_discriminator()\n",
    "#         self.generator = define_generator(latent_dim)\n",
    "#         self.gan = define_gan(self.generator, self.discriminator)\n",
    "        \n",
    "    def build_conv(self):\n",
    "        conv = Sequential(name='conv')\n",
    "        conv.add(Conv2D(128, kernel_size=4, strides=2, input_shape=(self.width, self.height, self.channels), padding=\"same\"))\n",
    "#         conv.add(BatchNormalization())\n",
    "        conv.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        conv.add(Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "#         conv.add(BatchNormalization())\n",
    "        conv.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        conv.add(Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "#         conv.add(BatchNormalization())\n",
    "        conv.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        conv.add(Flatten())\n",
    "    #     conv.add(Dense(256))\n",
    "    #     conv.add(LeakyReLU(alpha=0.1))\n",
    "    #     conv.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        conv.add(Dropout(0.4))\n",
    "\n",
    "        conv.add(Dense(1))\n",
    "        return conv\n",
    "\n",
    "\n",
    "    def build_classifier(self, conv):\n",
    "        model = Sequential(name='classifier')\n",
    "        model.add(conv)\n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self, conv):\n",
    "        model = Sequential(name='discriminator')\n",
    "        model.add(conv)\n",
    "\n",
    "        def predict1(output):\n",
    "            logexpsum = K.sum(K.exp(output), axis=-1, keepdims=True)\n",
    "            result = logexpsum / (logexpsum + 1.0)\n",
    "            return result\n",
    "\n",
    "        def predict(y):\n",
    "            p = 1.0 - (1.0 / (K.sum(K.exp(y), axis=-1, keepdims=True) + 1.0))\n",
    "            return p\n",
    "\n",
    "#         model.add(Lambda(predict))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_gen(self):\n",
    "        model = Sequential(name='generator')\n",
    "\n",
    "        model.add(Dense(32*int(self.width/4)*int(self.height/4), input_dim=self.latent_dim))\n",
    "    #     model.add(BatchNormalization(momentum=0.9, epsilon=0.00002))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Reshape((int(self.width/4),int(self.height/4), 32)))\n",
    "\n",
    "        model.add(UpSampling2D(interpolation='nearest'))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    #     model.add(BatchNormalization(momentum=0.9, epsilon=0.00002))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(UpSampling2D(interpolation='nearest'))\n",
    "        model.add(Conv2D(48, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    #     model.add(BatchNormalization(momentum=0.9, epsilon=0.00002))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(UpSampling2D(interpolation='nearest'))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    #     model.add(BatchNormalization(momentum=0.9, epsilon=0.00002))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(UpSampling2D(interpolation='nearest'))\n",
    "        model.add(Conv2D(16, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    #     model.add(BatchNormalization(momentum=0.9, epsilon=0.00002))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Conv2D(8, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    #     model.add(BatchNormalization(momentum=0.9, epsilon=0.00002))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(UpSampling2D(interpolation='nearest'))\n",
    "        model.add(Conv2D(16, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    #     model.add(BatchNormalization(momentum=0.9, epsilon=0.00002))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def build_gan(self, gen, dis):\n",
    "        model = Sequential(name='gan')\n",
    "        model.add(gen)\n",
    "        model.add(dis)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "\n",
    "        conv = self.build_conv()\n",
    "        clf = self.build_classifier(conv)\n",
    "        dis = self.build_discriminator(conv)\n",
    "        gen = self.build_gen()\n",
    "\n",
    "#         opt = Adam(clipvalue=1.0, lr=0.0002, beta_1=0.5)\n",
    "#         opt_weak = Adam(clipvalue=0.1, lr=0.0001)\n",
    "\n",
    "#         opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "#         opt_weak = Adam(lr=0.0001, beta_1=0.5)\n",
    "\n",
    "#         clf_loss = 'sparse_categorical_crossentropy'\n",
    "#         clf.compile(loss=clf_loss, metrics=['accuracy'], optimizer=opt_weak)\n",
    "        clf.compile(loss=wasserstein_loss,optimizer=RMSprop(lr=0.00005), metrics=['accuracy'])\n",
    "\n",
    "#         dis_loss = 'binary_crossentropy'\n",
    "#         dis.compile(loss=dis_loss, optimizer=opt)\n",
    "        dis.compile(loss=wasserstein_loss,optimizer=RMSprop(lr=0.00005))\n",
    "\n",
    "        dis.trainable = False\n",
    "        gan = self.build_gan(gen, dis)\n",
    "#         gan_loss = 'binary_crossentropy'\n",
    "#         gan.compile(loss=gan_loss, optimizer=opt)\n",
    "        gan.compile(loss=wasserstein_loss,optimizer=RMSprop(lr=0.00005))\n",
    "\n",
    "        return clf, dis, gen, gan\n",
    "\n",
    "    def train(self, epochs, X_train, y_train, X, X_val, y_val, batch_size=28):\n",
    "        # d_real = np.ones((batch_size, 1))\n",
    "        # d_fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        \n",
    "        g_batch_size = int(batch_size*2)\n",
    "\n",
    "\n",
    "        self.clf_loss = []\n",
    "        self.clf_acc = []\n",
    "        self.clf_loss_val = []\n",
    "        self.clf_acc_val = []\n",
    "\n",
    "        self.g_loss = []\n",
    "        self.d_loss_real=[]\n",
    "        self.d_loss_fake=[]\n",
    "\n",
    "#         self.d_acc=[]\n",
    "\n",
    "        idx_range = np.arange(X_train.shape[0])\n",
    "\n",
    "        for e in range(epochs):\n",
    "            clf_loss = []\n",
    "            clf_acc = []\n",
    "            g_loss = []\n",
    "            d_loss_fake = []\n",
    "            d_loss_real = []\n",
    "            for idx in tqdm(np.array_split(shuffle(idx_range), idx_range.shape[0]/(batch_size/2)), desc=\"epoch \"+str(e)):\n",
    "                y_clf = y_train[idx]\n",
    "                y_clf[y_clf==0] = -1\n",
    "                loss, acc = self.classifier.train_on_batch(X_train[idx], y_clf)\n",
    "                clf_loss.append(loss)\n",
    "                clf_acc.append(acc)\n",
    "\n",
    "                self.discriminator.trainable = True\n",
    "                idx_u = np.random.choice(X.shape[0], idx.shape[0], replace=False)\n",
    "                d_loss_real.append(self.discriminator.train_on_batch(X[idx_u], -np.ones((idx.shape[0], 1))))\n",
    "\n",
    "                noise = np.random.normal(0, 1, (idx.shape[0], self.latent_dim))\n",
    "                x_fake = self.generator.predict(noise)\n",
    "                d_loss_fake.append(self.discriminator.train_on_batch(x_fake, np.ones((idx.shape[0], 1))))\n",
    "                \n",
    "                weights = [np.clip(w, -0.01, 0.01) for w in self.discriminator.get_weights()]\n",
    "                self.discriminator.set_weights(weights)\n",
    "                self.discriminator.trainable = False\n",
    "\n",
    "                noise = np.random.normal(0, 1, (idx.shape[0]*3, self.latent_dim))\n",
    "                g_loss.append(self.gan.train_on_batch(noise, -np.ones((noise.shape[0], 1))))\n",
    "\n",
    "            loss, acc = self.classifier.evaluate(X_val, y_val, batch_size=batch_size, verbose=0)\n",
    "            self.clf_loss_val.append(loss)\n",
    "            self.clf_acc_val.append(acc)\n",
    "            self.clf_loss.append(sum(clf_loss)/len(clf_loss))\n",
    "            self.clf_acc.append(sum(clf_acc)/len(clf_acc))\n",
    "            self.g_loss.append(sum(g_loss)/len(g_loss))\n",
    "            self.d_loss_real.append(sum(d_loss_real)/len(d_loss_real))\n",
    "            self.d_loss_fake.append(sum(d_loss_fake)/len(d_loss_fake))\n",
    "            \n",
    "            print(\"D real: %f; D fake %f; G loss %f\"%(self.d_loss_real[-1], self.d_loss_fake[-1], self.g_loss[-1]))\n",
    "            print(\"Clf Loss: %f; acc: %.2f\"%(self.clf_loss[-1], self.clf_acc[-1]))\n",
    "            print(\"Val Loss: %f; acc: %.2f\"%(loss, acc))\n",
    "\n",
    "            if e%10 == 0:\n",
    "                plot_samples(self.generator.predict(np.random.normal(0, 1, (16, self.latent_dim))), 'SSL_WGAN', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_sgan = ImprovedSGAN(64, 64, 3, 2)\n",
    "improved_sgan.train(10000, x_train_small, y_train_small, x_train, x_test, y_test, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
