{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID' \n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "# from torchvision import datasets, transforms\n",
    "# from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    x_train = (h5py.File('camelyonpatch_level_2_split_train_x.h5', 'r')['x'][:, 16:80,16:80] - 127.5) / 127.5\n",
    "    y_train = h5py.File('camelyonpatch_level_2_split_train_y.h5', 'r')['y'][:].reshape(-1,1)\n",
    "    x_test = (h5py.File('camelyonpatch_level_2_split_test_x.h5', 'r')['x'][:, 16:80,16:80] - 127.5) / 127.5\n",
    "    y_test = h5py.File('camelyonpatch_level_2_split_test_y.h5', 'r')['y'][:].reshape(-1,1)\n",
    "    x_valid = (h5py.File('camelyonpatch_level_2_split_valid_x.h5', 'r')['x'][:, 16:80,16:80] - 127.5) / 127.5\n",
    "    y_valid = h5py.File('camelyonpatch_level_2_split_valid_y.h5', 'r')['y'][:].reshape(-1,1)\n",
    "              \n",
    "    return x_train, y_train, x_test, y_test, x_valid, y_valid\n",
    "\n",
    "def plot_samples(samples, folder=None, epoch=None, i=None):\n",
    "    rt = int(np.sqrt(samples.shape[0]))\n",
    "    r, c = rt, rt\n",
    "    # r, c = 6, 12\n",
    "\n",
    "    generatedImage = 0.5 * samples + 0.5\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "    axs = [fig.add_subplot(r,c,i+1) for i in range(r*c)]\n",
    "    cnt = 0\n",
    "    for ax in axs:\n",
    "        ax.imshow(generatedImage[cnt],interpolation='nearest')\n",
    "        ax.axis('off')\n",
    "        ax.set_aspect('equal')\n",
    "        cnt+=1\n",
    "    fig.subplots_adjust(wspace=.004, hspace=.02)\n",
    "\n",
    "    if folder:\n",
    "        path = 'results/'+folder+'/samples'\n",
    "        if not os.path.exists('results'):\n",
    "            os.mkdir('results')\n",
    "        if not os.path.exists('results/'+folder):\n",
    "            os.mkdir('results/'+folder)\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        step = \"\"\n",
    "        if i:\n",
    "            step = '_'+str(i)\n",
    "        fig.savefig(path+'/epoch_%d%s.png' % (epoch, step))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aray/pathology_gan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([32733., 32803.]),\n",
       " array([0. , 0.5, 1. ], dtype=float32),\n",
       " <a list of 2 Patch objects>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%cd \"~/pathology_gan\"\n",
    "\n",
    "x_train, y_train, x_test, y_test, x_valid, y_valid = load_data()\n",
    "\n",
    "x_train = torch.from_numpy(np.moveaxis(x_train.astype(np.float32), -1, 1))\n",
    "x_test  = torch.from_numpy(np.moveaxis(x_test.astype(np.float32), -1, 1))\n",
    "x_valid  = torch.from_numpy(np.moveaxis(x_valid.astype(np.float32), -1, 1))\n",
    "\n",
    "y_train = torch.from_numpy(y_train).float() \n",
    "y_test = torch.from_numpy(y_test).float() \n",
    "y_valid = torch.from_numpy(y_valid).float() \n",
    "\n",
    "# X = torch.from_numpy(np.moveaxis(np.concatenate([x_train, x_test, x_valid]).astype(np.float32), -1, 1))\n",
    "# y = torch.from_numpy(np.concatenate([y_train, y_test, y_valid]))\n",
    "\n",
    "# trainloader = DataLoader(TensorDataset(x_train, y_train), batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
    "validloader = DataLoader(TensorDataset(x_valid, y_valid), batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "percent = int(x_train.shape[0]*.25)\n",
    "np.random.seed(17)\n",
    "idx_small = np.random.choice(range(x_train.shape[0]), percent, replace=False)\n",
    "\n",
    "x_train_small = x_train[idx_small]\n",
    "y_train_small = y_train[idx_small]\n",
    "trainloader = DataLoader(TensorDataset(x_train_small, y_train_small), batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "plt.hist(y_train_small.numpy(), bins=2)\n",
    "# print(percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_small = y_train_small.type(torch.LongTensor)\n",
    "y_valid=y_valid.type(torch.LongTensor)\n",
    "trainloader = DataLoader(TensorDataset(x_train_small, y_train_small), batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
    "validloader = DataLoader(TensorDataset(x_valid, y_valid), batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = np.random.normal(0,1,size=(16,100))\n",
    "# imgs = saved_actor.predict(noise)\n",
    "# plot_samples(imgs)\n",
    "\n",
    "# imgs_real = .5+.5*np.moveaxis(x_train[np.random.choice(range(x_train.shape[0]), 12*6, replace=False)], -1, 1)\n",
    "# plot_samples(x_train[np.random.choice(range(x_train.shape[0]), 12*6, replace=False)])\n",
    "# print(imgs_real.shape)\n",
    "# plt.figure(figsize=(24,12))\n",
    "# plt.imshow(np.transpose(make_grid(torch.from_numpy(imgs_real), nrow=12, ).numpy(), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, w, h, c, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        self.c = c\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.input = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32 * 16 * 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32 * 16 * 16),\n",
    "        )\n",
    "        \n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.Upsample(size=[32, 32], mode='nearest'),\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            nn.Upsample(size=[64, 64], mode='nearest'),\n",
    "            nn.Conv2d(64, 48, 4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(48),\n",
    "            \n",
    "            nn.Upsample(size=[64, 64], mode='nearest'),\n",
    "            nn.Conv2d(48, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(32),\n",
    "\n",
    "            nn.Upsample(size=[128, 128], mode='nearest'),\n",
    "            nn.Conv2d(32, 16, 4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(16),\n",
    "\n",
    "            nn.Conv2d(16, 8, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(8),\n",
    "\n",
    "            nn.Upsample(size=[128, 128], mode='nearest'),\n",
    "            nn.Conv2d(8, 16, 4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(16),\n",
    "            \n",
    "            nn.Conv2d(16, c, 3, stride=1, padding=1),\n",
    "            nn.Tanh()            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.input(x)\n",
    "        # print(output.shape)\n",
    "        output = output.view(-1, 32, 16, 16)\n",
    "        # print(output.shape)\n",
    "        return self.deconv(output) #.view(-1, self.w, self.h, self.c)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, h, w, c):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        self.c = c\n",
    "        n_filters = 32\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            weight_norm(nn.Conv2d(c, n_filters, 4, stride=2, padding=1), name='weight'),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "            \n",
    "            weight_norm(nn.Conv2d(n_filters, 2*n_filters, 4, stride=2, padding=1), name='weight'),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "                        \n",
    "            weight_norm(nn.Conv2d(2*n_filters, 4*n_filters, 4, stride=2, padding=1), name='weight'),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc = weight_norm(nn.Linear(4*n_filters*int(w/2**3)*int(h/2**3), 2, bias=True), name='weight')\n",
    "\n",
    "    def forward(self, x, clf=True, dropout=True):\n",
    "        flatten = self.conv(x)\n",
    "        if dropout:\n",
    "            flatten = F.dropout(flatten, p=0.5)\n",
    "        \n",
    "        out = self.fc(flatten)\n",
    "        \n",
    "        if clf:\n",
    "            return F.softmax(out, dim=1)\n",
    "\n",
    "#         expsum = torch.exp(out).sum(dim=1)\n",
    "#         out = expsum/(expsum+1)\n",
    "        return out, flatten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSL_WGAN:\n",
    "    def __init__(self, w, h, c, model_name, latent_dim=100):\n",
    "        self.model_name = model_name\n",
    "        self.latent_dim = latent_dim \n",
    "        self.lambda_gp = 10\n",
    "        self.lambda_ct = 2\n",
    "        self.d_iterations = 5\n",
    "        self.print_every = 10\n",
    "        # CT multiplier\n",
    "        self.M = .1\n",
    "        self.use_cuda = True\n",
    "        self.D = Discriminator(w, h, c)\n",
    "        self.G = Generator(w, h, c, latent_dim)\n",
    "\n",
    "        lr = 1e-4\n",
    "        betas = (.9, .99)\n",
    "\n",
    "        self.Clf_opt = optim.Adam(self.D.parameters(), lr=lr/2, betas=betas)\n",
    "        self.Clf_criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.D_opt = optim.Adam(self.D.parameters(), lr=lr, betas=betas)\n",
    "        self.G_opt = optim.Adam(self.G.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "\n",
    "        if self.use_cuda:\n",
    "            self.D = self.D.cuda()\n",
    "            self.G = self.G.cuda()\n",
    "\n",
    "        if not os.path.exists('results/'+model_name):\n",
    "            os.mkdir('results/'+model_name)\n",
    "\n",
    "    def train(self, train_loader, validation_loader, X_unlabled, epochs, save_training_gif=True):\n",
    "        writer = SummaryWriter('/home/aray/runs/'+self.model_name)  \n",
    "        if save_training_gif:\n",
    "            # Fix latents to see how image generation improves during training\n",
    "            self.fixed_latents = torch.randn((12*6, self.latent_dim))\n",
    "            if self.use_cuda:\n",
    "                self.fixed_latents = self.fixed_latents.cuda()\n",
    "            self.training_progress_images = []\n",
    "\n",
    "        self.stats = {\n",
    "            'clf_loss': [],\n",
    "            'clf_acc': [],\n",
    "            'clf_loss_val': [],\n",
    "            'clf_acc_val': [],\n",
    "            'g_loss': [],\n",
    "            'd_loss': [],\n",
    "            'd_loss_real': [],\n",
    "            'd_loss_fake': [],\n",
    "            'gp': [],\n",
    "            'ct': [],\n",
    "            \n",
    "        }\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            clf_loss = []\n",
    "            clf_acc = []\n",
    "            g_loss = []\n",
    "            d_loss = []\n",
    "            d_loss_fake = []\n",
    "            d_loss_real = []\n",
    "            gradient_penalty = []\n",
    "            consistency_term = []\n",
    "\n",
    "#             for i, data in tqdm(enumerate(train_loader), desc=\"epoch \"+str(epoch)):\n",
    "            for i, data in enumerate(train_loader):\n",
    "#                 if i%((n_samples//128)//100)==0:\n",
    "#                     print(\".\", end=\"\", flush=True)\n",
    "                X = data[0]\n",
    "                y = data[1].view(-1)\n",
    "                batch_size = X.shape[0]\n",
    "                idx_u = np.random.choice(X_unlabled.shape[0], batch_size, replace=False)\n",
    "                X_u = X_unlabled[idx_u]\n",
    "                if self.use_cuda:\n",
    "                    X_u = X_u.cuda()\n",
    "                    X = X.cuda()\n",
    "                    y = y.cuda()\n",
    "                    \n",
    "                loss, acc = self._train_Clf(X, y)\n",
    "                clf_loss.append(loss)\n",
    "                clf_acc.append(acc)                                \n",
    "                    \n",
    "                ct, loss = self._train_D(X_u)\n",
    "#                 d_loss_real.append(r)\n",
    "#                 d_loss_fake.append(g)\n",
    "#                 gradient_penalty.append(gp)\n",
    "                consistency_term.append(ct)\n",
    "                d_loss.append(loss)\n",
    "                \n",
    "                # Only update generator every |d_iterations| iterations\n",
    "#                 if i % self.d_iterations == 0:\n",
    "                g_loss.append(self._train_G1(X_u))\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                acc_val = self._eval_Clf(validation_loader)\n",
    "                clf_loss_m = sum(clf_loss)/len(clf_loss)\n",
    "                clf_acc_m = sum(clf_acc)/len(clf_acc)     \n",
    "                g_loss_m = sum(g_loss)/len(g_loss)\n",
    "                d_loss_m = sum(d_loss)/len(d_loss)\n",
    "#                 real_m = sum(d_loss_real)/len(d_loss_real)\n",
    "#                 fake_m = sum(d_loss_fake)/len(d_loss_fake)\n",
    "#                 gp_m = sum(gradient_penalty)/len(gradient_penalty)\n",
    "                ct_m = sum(consistency_term)/len(consistency_term)\n",
    "                self.stats['clf_loss'].append(clf_loss_m)\n",
    "                self.stats['clf_acc'].append(clf_acc_m)\n",
    "    #             self.stats['clf_loss_val'].append(loss_val)\n",
    "                self.stats['clf_acc_val'].append(acc_val)\n",
    "                self.stats['g_loss'].append(g_loss_m)\n",
    "                self.stats['d_loss'].append(d_loss_m)\n",
    "#                 self.stats['d_loss_real'].append(real_m)\n",
    "#                 self.stats['d_loss_fake'].append(fake_m)\n",
    "#                 self.stats['gp'].append(gp_m)\n",
    "                self.stats['ct'].append(ct_m)\n",
    "                print(\"Epoch: %d, G loss: %f\"%(epoch, g_loss_m))\n",
    "                print(\"Clf loss: %f, acc: %.3f, acc_val: %.3f\"%(clf_loss_m, clf_acc_m, acc_val))\n",
    "                print(\"D loss: %f; ct: %f\"%(d_loss_m, ct_m))\n",
    " \n",
    "                writer.add_scalar('wgan_weight_nomr_ct', ct_m, epoch)\n",
    "                writer.add_scalar('wgan_weight_nomr_d_loss', d_loss_m, epoch)\n",
    "                writer.add_scalar('wgan_weight_nomr_g_loss', g_loss_m, epoch)\n",
    "                writer.add_scalar('wgan_weight_nomr_clf_loss', clf_loss_m, epoch)\n",
    "                writer.add_scalar('wgan_weight_nomr_clf_acc', clf_acc_m, epoch)\n",
    "                writer.add_scalar('wgan_weight_nomr_acc_val', acc_val, epoch)\n",
    "\n",
    "#             if epoch % 50 == 0:\n",
    "#                 with torch.no_grad():\n",
    "#                     self.G.eval()\n",
    "# #                     self.D.eval()\n",
    "# #                     print(self.D(X_unlabled[:10].cuda()).view(-1))\n",
    "# #                     self.D.train()\n",
    "#                     imgs = self.G(torch.randn((16, self.latent_dim)).cuda()).cpu().numpy()\n",
    "#                     plot_samples(np.moveaxis(imgs, 1,-1), self.model_name, epoch)\n",
    "#                     self.G.train()\n",
    "\n",
    "            if  epoch % 10 == 0 and save_training_gif:\n",
    "                with torch.no_grad():\n",
    "                    self.G.eval()\n",
    "                    img_grid = vutils.make_grid(self.G(self.fixed_latents).cpu(), nrow=12).numpy()\n",
    "                    # (width, height, channels)\n",
    "                    img_grid = .5+.5*np.transpose(img_grid, (1, 2, 0))\n",
    "                    self.training_progress_images.append(img_grid)\n",
    "                    writer.add_image('wgan_weight_nomr_images', img_grid, epoch, dataformats='HWC')\n",
    "                    self.G.train()\n",
    "\n",
    "            # if i % self.print_every == 0:\n",
    "            #     print(\"Iteration {}\".format(i + 1))\n",
    "            #     print(\"D: {}\".format(self.losses['D'][-1]))\n",
    "            #     print(\"GP: {}\".format(self.losses['GP'][-1]))\n",
    "            #     print(\"Gradient norm: {}\".format(self.losses['gradient_norm'][-1]))\n",
    "            #     if self.num_steps > self.critic_iterations:\n",
    "            #         print(\"G: {}\".format(self.losses['G'][-1]))\n",
    "\n",
    "        if save_training_gif:\n",
    "            imageio.mimsave('results/'+self.model_name+'/training_{}_epochs.gif'.format(epochs), self.training_progress_images)\n",
    "\n",
    "    def _eval_Clf(self, validation_loader):\n",
    "        self.D.eval()        \n",
    "        with torch.no_grad():\n",
    "            acc = .0\n",
    "            for i, data in enumerate(validation_loader):\n",
    "                X = data[0]\n",
    "                y = data[1].view(-1)\n",
    "                if self.use_cuda:\n",
    "                    X = X.cuda()\n",
    "                    y = y.cuda()\n",
    "                predicted = torch.argmax(self.D(X), dim=1)\n",
    "                acc+=(predicted == y).sum()/float(predicted.shape[0])       \n",
    "        self.D.train()\n",
    "        return (acc/(i+1)).detach().item()\n",
    "\n",
    "    def _train_Clf(self, data, labels):\n",
    "        self.Clf_opt.zero_grad()\n",
    "        \n",
    "        predicted = self.D(data, clf=True, dropout=True)\n",
    "    \n",
    "        loss = self.Clf_criterion(predicted, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.Clf_opt.step()    \n",
    "      \n",
    "        acc = (torch.argmax(predicted.detach().cpu(), dim=1) == labels.detach().cpu()).sum()/float(predicted.shape[0])\n",
    "\n",
    "        return loss.detach().item(), acc\n",
    "        \n",
    "#     def _D_loss(self, out):\n",
    "#         expsum = torch.exp(out).sum(dim=1)\n",
    "#         return expsum/(expsum+1)    \n",
    "    \n",
    "    def _train_D(self, data):\n",
    "        self.D_opt.zero_grad()\n",
    "\n",
    "        batch_size = data.shape[0]\n",
    "        generated_data = self.sample_generator(batch_size)\n",
    "        \n",
    "        real_fc1, real_flatten1 = self.D(data, clf=False)\n",
    "        real_fc2, real_flatten2 = self.D(data, clf=False)\n",
    "        gen_fc, _ = self.D(generated_data, clf=False)\n",
    "       \n",
    "        ct1 = ((torch.softmax(real_fc1, dim=1) - torch.softmax(real_fc2, dim=1))**2).mean()\n",
    "        ct2 = ((torch.softmax(real_flatten1, dim=1) - torch.softmax(real_flatten2, dim=1))**2).mean()\n",
    "        \n",
    "        ct = 0.5*ct1 + 0.05*ct2\n",
    "\n",
    "        real_lse = torch.logsumexp(real_fc1, dim=1)\n",
    "        \n",
    "        real_mean = F.softplus(real_lse).mean()\n",
    "        gen_mean = F.softplus(torch.logsumexp(gen_fc, dim=1)).mean()\n",
    "\n",
    "        d_loss = ct + 0.5*(real_mean + gen_mean - real_lse.mean())\n",
    "        \n",
    "        d_loss.backward()\n",
    "        self.D_opt.step()\n",
    "\n",
    "\n",
    "        return ct.detach().item(), d_loss.detach().item()\n",
    "\n",
    "    \n",
    "    def _train_G1(self, real_data):\n",
    "        for p in self.D.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        self.G_opt.zero_grad()\n",
    "\n",
    "        batch_size = real_data.shape[0]\n",
    "        generated_data = self.sample_generator(batch_size)\n",
    "\n",
    "        _, d_real_output = self.D(real_data, clf=False, dropout=False)\n",
    "        _, d_gen_output = self.D(generated_data, clf=False, dropout=False)\n",
    "\n",
    "        g_loss = ((d_gen_output.mean(dim=0)-d_real_output.mean(dim=0))**2).mean()\n",
    "        g_loss.backward()\n",
    "        self.G_opt.step()\n",
    "        \n",
    "        for p in self.D.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        return g_loss.detach().item()    \n",
    "\n",
    "    def sample_generator(self, num_samples):\n",
    "        latent_samples = torch.randn((num_samples, self.latent_dim), requires_grad=True)\n",
    "        if self.use_cuda:\n",
    "            latent_samples = latent_samples.cuda()\n",
    "        generated_data = self.G(latent_samples)\n",
    "        return generated_data\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        generated_data = self.sample_generator(num_samples)\n",
    "        # Remove color channel\n",
    "        return generated_data.data.cpu().numpy()[:, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_stats(model):\n",
    "    with open('results/'+model.model_name+'/clf_acc.csv', 'w', newline='') as myfile:\n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow(model.stats['clf_acc'])\n",
    "\n",
    "    with open('results/'+model.model_name+'/clf_loss.csv', 'w', newline='') as myfile:\n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow(model.stats['clf_loss'])\n",
    "\n",
    "    with open('results/'+model.model_name+'/clf_acc_val.csv', 'w', newline='') as myfile:\n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow(model.stats['clf_acc_val'])\n",
    "        \n",
    "    with open('results/'+model.model_name+'/d_loss.csv', 'w', newline='') as myfile:\n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow(model.stats['d_loss'])\n",
    "\n",
    "    with open('results/'+model.model_name+'/g_loss.csv', 'w', newline='') as myfile:\n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow(model.stats['g_loss'])\n",
    "\n",
    "    with open('results/'+model.model_name+'/ct.csv', 'w', newline='') as myfile:\n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow(model.stats['ct'])\n",
    "\n",
    "def save_plots(model):\n",
    "    path = 'results/'+model.model_name\n",
    "    \n",
    "    fig = plt.figure(figsize=(18,5))\n",
    "    plt.plot(wgan_ct.stats['ct'], label='CT')\n",
    "    plt.xlabel('Epochs x10')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    fig.savefig(path+'/ct.png')\n",
    "\n",
    "    fig = plt.figure(figsize=(18,5))\n",
    "    plt.plot(wgan_ct.stats['d_loss'], label='D')\n",
    "    plt.plot(wgan_ct.stats['g_loss'], label='G')\n",
    "    plt.xlabel('Epochs x10')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    fig.savefig(path+'/gan_loss.png')\n",
    "\n",
    "    fig = plt.figure(figsize=(18,5))\n",
    "    plt.plot(wgan_ct.stats['clf_loss'], label='Clf train')\n",
    "    plt.xlabel('Epochs x10')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    fig.savefig(path+'/clf_train_loss.png')\n",
    "\n",
    "    fig = plt.figure(figsize=(18,5))\n",
    "    acc_max = np.max(wgan_ct.stats['clf_acc_val'])\n",
    "    acc_max_e = np.argmax(wgan_ct.stats['clf_acc_val'])\n",
    "    plt.axhline(y=acc_max, color='k', linestyle='-', alpha=0.2, linewidth=1)\n",
    "    plt.axvline(x=acc_max_e, color='k', linestyle='-', alpha=0.2, linewidth=1)\n",
    "    plt.plot(wgan_ct.stats['clf_acc'], label='Train')\n",
    "    plt.plot(wgan_ct.stats['clf_acc_val'], label='Validation')\n",
    "    plt.xlabel('Epochs x10')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend()\n",
    "    fig.savefig(path+'/clf_acc.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, G loss: 0.009601\n",
      "Clf loss: 0.625997, acc: 0.660, acc_val: 0.697\n",
      "D loss: 0.344599; ct: 0.011591\n"
     ]
    }
   ],
   "source": [
    "wgan_ct = SSL_WGAN(64, 64, 3, 'ssl_wgan_weight_norm_25')\n",
    "wgan_ct.train(trainloader, validloader, x_train, 2000, x_train.shape[0])\n",
    "save_stats(wgan_ct)\n",
    "save_plots(wgan_ct)\n",
    "torch.save(wgan_ct.D.state_dict(), 'results/'+wgan_ct.model_name+'/D.pt')\n",
    "torch.save(wgan_ct.G.state_dict(), 'results/'+wgan_ct.model_name+'/G.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
